[
  {
    "objectID": "posts/6.LinearModelSelection_and_Regularization.html",
    "href": "posts/6.LinearModelSelection_and_Regularization.html",
    "title": "6. Linear Model Selection and Regularization",
    "section": "",
    "text": "1. imports\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.pyplot import subplots\nfrom statsmodels.api import OLS\nimport sklearn.model_selection as skm\nimport sklearn.linear_model as skl\nfrom sklearn.preprocessing import StandardScaler\nfrom ISLP import load_data\nfrom ISLP.models import ModelSpec as MS\nfrom functools import partial\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_decomposition import PLSRegression\nfrom ISLP.models import (Stepwise, sklearn_selected, sklearn_selection_path)\nfrom l0bnb import fit_path\n\n\nHitters = load_data('Hitters')\nnp.isnan(Hitters['Salary']).sum()\nHitters = Hitters.dropna(); Hitters.shape\n\n(263, 20)\n\n\n\n\n2. 단계적 선택법\n- Cp계산법 (변형이 있음) 및 함수 설정\n\ndef nCp(sigma2, estimator, X, Y):\n  n, p = X.shape\n  Yhat = estimator.predict(X)\n  RSS = np.sum((Y - Yhat)**2)\n  return -(RSS + 2 * p * sigma2) / n\n\ndesign = MS(Hitters.columns.drop('Salary')).fit(Hitters)\nY = np.array(Hitters['Salary'])\nX = design.transform(Hitters)\nsigma2 = OLS(Y,X).fit().scale\n\nneg_Cp = partial(nCp, sigma2)\nprint(neg_Cp)\n\nfunctools.partial(&lt;function nCp at 0x7f4e38e84d30&gt;, 99591.35617968219)\n\n\n- 단계적 선택 방법을 적용(사용하는 측도는 일반적으로 P-value)\n\nstrategy = Stepwise.first_peak(design, direction='forward', max_terms=len(design.terms))\nhitters_MSE = sklearn_selected(OLS, strategy)\nhitters_MSE.fit(Hitters, Y)\nhitters_MSE.selected_state_\n\n('Assists',\n 'AtBat',\n 'CAtBat',\n 'CHits',\n 'CHmRun',\n 'CRBI',\n 'CRuns',\n 'CWalks',\n 'Division',\n 'Errors',\n 'Hits',\n 'HmRun',\n 'League',\n 'NewLeague',\n 'PutOuts',\n 'RBI',\n 'Runs',\n 'Walks',\n 'Years')\n\n\n- Cp를 이용한 단계적 선택\n\nhitters_Cp = sklearn_selected(OLS, strategy, scoring=neg_Cp)\nhitters_Cp.fit(Hitters, Y)\nhitters_Cp.selected_state_\n\n('Assists',\n 'AtBat',\n 'CAtBat',\n 'CRBI',\n 'CRuns',\n 'CWalks',\n 'Division',\n 'Hits',\n 'PutOuts',\n 'Walks')\n\n\n- 단계적 선택 과정에서 모든 예측치를 행렬로 모으기\n\nstrategy = Stepwise.fixed_steps(design, len(design.terms), direction='forward')\nfull_path = sklearn_selection_path(OLS, strategy)\nfull_path.fit(Hitters, Y)\nYhat_in = full_path.predict(Hitters)\nYhat_in.shape\n\n(263, 20)\n\n\n- 위에서 저장한 예측치(룬련 데이터)로 예측오차를 계산해서 그리기\n\nmse_fig, ax = subplots(figsize=(8,8))\ninsample_mse = ((Yhat_in - Y[:,None])**2).mean(0)\nn_steps = insample_mse.shape[0]\nax.plot(np.arange(n_steps), insample_mse,\n        'k', # color black\n        label='In-sample')\nax.set_ylabel('MSE', fontsize=20)\nax.set_xlabel('# steps of forward stepwise', fontsize=20)\nax.set_xticks(np.arange(n_steps)[::2])\nax.legend()\nax.set_ylim([50000,250000]);\n\n\n\n\n\n\n\n\n\n\n3. 교차검증의 활용\n- 10 folder CV를 통해서 예측된 값들을 모아놓음(훈련과 검증을 구분했음!!)\n\nK=10\nkfold = skm.KFold(K, random_state=0, shuffle=True)\nYhat_cv = skm.cross_val_predict(full_path, Hitters, Y, cv=kfold)\nprint(Yhat_cv.shape)\n\n(263, 20)\n\n\n- 위의 결과에 대한 훈련오차와 교차검증 오차\n\ncv_mse = []\nfor train_idx, test_idx in kfold.split(Y):\n  errors = (Yhat_cv[test_idx] - Y[test_idx ,None])**2\n  cv_mse.append(errors.mean(0)) # column means\ncv_mse = np.array(cv_mse).T\ncv_mse.shape\n\nax.errorbar(np.arange(n_steps),\ncv_mse.mean(1),\ncv_mse.std(1) / np.sqrt(K), # 교차검증오차의 표준오차\nlabel='Cross -validated', c='r')\nax.set_ylim ([50000 ,250000])\nax.legend()\nmse_fig\n\n\n\n\n\n\n\n\n- 훈련 예측 오차와 Validation set 을 이용한 평가 예측 오차\n\nvalidation = skm.ShuffleSplit(n_splits=1, test_size=0.2,\n                             random_state=0)\nfor train_idx, test_idx in validation.split(Y):\n  full_path.fit(Hitters.iloc[train_idx], Y[train_idx])\n  Yhat_val = full_path.predict(Hitters.iloc[test_idx])\n  errors = (Yhat_val - Y[test_idx ,None])**2\n  validation_mse = errors.mean(0)\n\nax.plot(np.arange(n_steps), validation_mse ,\n 'b--', # color blue, broken line\n label='Validation')\nax.set_xticks(np.arange(n_steps)[::2])\nax.set_ylim ([50000 ,250000])\nax.legend()\nmse_fig\n\n\n\n\n\n\n\n\n\n\n4. 축소 알고리즘\n- Ridge 회귀분석\n\nXs = X - X.mean(0)[None ,:]\nX_scale = X.std(0)\nXs = Xs / X_scale[None ,:]\n\nlambdas = 10**np.linspace(8, -2, 100) / Y.std()\nsoln_array = skl.ElasticNet.path(Xs, Y, l1_ratio=0., alphas=lambdas)[1]\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[25], line 1\n----&gt; 1 Xs = X - X.mean(0)[None ,:]\n      2 X_scale = X.std(0)\n      3 Xs = Xs / X_scale[None ,:]\n\nFile ~/anaconda3/envs/pypy/lib/python3.10/site-packages/pandas/core/series.py:1153, in Series.__getitem__(self, key)\n   1150     key = np.asarray(key, dtype=bool)\n   1151     return self._get_rows_with_mask(key)\n-&gt; 1153 return self._get_with(key)\n\nFile ~/anaconda3/envs/pypy/lib/python3.10/site-packages/pandas/core/series.py:1163, in Series._get_with(self, key)\n   1158     raise TypeError(\n   1159         \"Indexing a Series with DataFrame is not \"\n   1160         \"supported, use the appropriate DataFrame column\"\n   1161     )\n   1162 elif isinstance(key, tuple):\n-&gt; 1163     return self._get_values_tuple(key)\n   1165 elif not is_list_like(key):\n   1166     # e.g. scalars that aren't recognized by lib.is_scalar, GH#32684\n   1167     return self.loc[key]\n\nFile ~/anaconda3/envs/pypy/lib/python3.10/site-packages/pandas/core/series.py:1203, in Series._get_values_tuple(self, key)\n   1198 if com.any_none(*key):\n   1199     # mpl compat if we look up e.g. ser[:, np.newaxis];\n   1200     #  see tests.series.timeseries.test_mpl_compat_hack\n   1201     # the asarray is needed to avoid returning a 2D DatetimeArray\n   1202     result = np.asarray(self._values[key])\n-&gt; 1203     disallow_ndim_indexing(result)\n   1204     return result\n   1206 if not isinstance(self.index, MultiIndex):\n\nFile ~/anaconda3/envs/pypy/lib/python3.10/site-packages/pandas/core/indexers/utils.py:341, in disallow_ndim_indexing(result)\n    333 \"\"\"\n    334 Helper function to disallow multi-dimensional indexing on 1D Series/Index.\n    335 \n   (...)\n    338 in GH#30588.\n    339 \"\"\"\n    340 if np.ndim(result) &gt; 1:\n--&gt; 341     raise ValueError(\n    342         \"Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer \"\n    343         \"supported. Convert to a numpy array before indexing instead.\"\n    344     )\n\nValueError: Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer supported. Convert to a numpy array before indexing instead."
  },
  {
    "objectID": "posts/5.sampling&resampling.html",
    "href": "posts/5.sampling&resampling.html",
    "title": "5. Sampling & Resampling",
    "section": "",
    "text": "1. imports\n\n#!pip install ISLP\nimport numpy as np\nimport statsmodels.api as sm\nfrom ISLP import load_data\nfrom ISLP.models import (ModelSpec as MS, summarize, poly)\n\nfrom sklearn.model_selection import train_test_split\n\n\nfrom functools import partial\nfrom sklearn.model_selection import (cross_validate , KFold , ShuffleSplit)\nfrom sklearn.base import clone\nfrom ISLP.models import sklearn_sm\n\n\n\n2. Validation and CV\n- Auto 데이터 로딩 및 데이터 쪼개기\n\ntest_size를 키우면 훈련 데이터가 줄어듦\nValidation 오차 확인\n\n\nAuto = load_data('Auto')\nAuto_train, Auto_valid = train_test_split(Auto, test_size=100, random_state=0)\n\nhp_mm = MS(['horsepower'])\nX_train = hp_mm.fit_transform(Auto_train)\ny_train = Auto_train['mpg']\nmodel = sm.OLS(y_train, X_train)\nresults = model.fit()\nprint(results.summary())\n\nX_valid = hp_mm.transform(Auto_valid)\ny_valid = Auto_valid['mpg']\nvalid_pred = results.predict(X_valid)\ntrain_pred = results.predict(X_train)\n\nprint(np.mean((y_train - train_pred)**2))\nprint(np.mean((y_valid - valid_pred)**2))\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.599\nModel:                            OLS   Adj. R-squared:                  0.598\nMethod:                 Least Squares   F-statistic:                     433.9\nDate:                Tue, 15 Apr 2025   Prob (F-statistic):           1.48e-59\nTime:                        01:05:33   Log-Likelihood:                -879.71\nNo. Observations:                 292   AIC:                             1763.\nDf Residuals:                     290   BIC:                             1771.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     39.8828      0.848     47.020      0.000      38.213      41.552\nhorsepower    -0.1587      0.008    -20.831      0.000      -0.174      -0.144\n==============================================================================\nOmnibus:                        8.415   Durbin-Watson:                   2.042\nProb(Omnibus):                  0.015   Jarque-Bera (JB):                8.304\nSkew:                           0.397   Prob(JB):                       0.0157\nKurtosis:                       3.226   Cond. No.                         327.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n24.228292976800535\n23.20193004710425\n\n\n- 훈련과 검증 오차를 확인하는 함수 생성\n\ndef evalMSE(terms, response , train , test):\n    mm = MS(terms)\n    X_train = mm.fit_transform(train)\n    y_train = train[response]\n    X_test = mm.transform(test)\n    y_test = test[response]\n    results = sm.OLS(y_train, X_train).fit()\n    test_pred = results.predict(X_test)\n    return np.mean((y_test - test_pred)**2)\n\n- 다항모형의 차수를 바꾸면서 검증 오차를 확인(Validation error for degrees)\n\nMSE = np.zeros(3)\nfor idx, degree in enumerate(range(1, 4)):\n    MSE[idx] = evalMSE([poly('horsepower', degree)], 'mpg', Auto_train, Auto_valid)\nMSE\n\narray([23.20193005, 16.99348987, 16.9506061 ])\n\n\n- 교차 검증 오차(leave-one-out)\n\nhp_model = sklearn_sm(sm.OLS, MS(['horsepower']))\nX, Y = Auto.drop(columns=['mpg']), Auto['mpg']\ncv_results = cross_validate(hp_model, X, Y, cv=Auto.shape[0])\ncv_err = np.mean(cv_results['test_score'])\ncv_err\n\n24.231513517929226\n\n\n- 다항식의 차수를 키웠을 때의 교차검증오차(leave-one-out)을 확인\n\ncv_error = np.zeros(5)\nH = np.array(Auto['horsepower'])\nM = sklearn_sm(sm.OLS)\n\nfor i, d in enumerate(range(1,6)):\n  X = np.power.outer(H, np.arange(d+1))\n  M_CV = cross_validate(M, X, Y, cv=Auto.shape[0])\n  cv_error[i] = np.mean(M_CV['test_score'])\n\ncv_error\n\narray([24.23151352, 19.24821312, 19.33498406, 19.42443036, 19.03323735])\n\n\n- 40 fold-CV 를 통해서 차수에 대한 예측오차를 확인함\n\nshupple=True : 무작위로 섞어라\nn_split=40 : 40개로 쪼개라\n1차에서 값이 크고 2차에서 값이 뚝 떨어진다\n\n\ncv_error = np.zeros(5)\n# 40개로 쪼개는 CV\ncv = KFold(n_splits=40, shuffle=True, random_state=0)\nprint(cv)\n\n# use same splits for each degree for i, d in enumerate(range(1,6)):\nfor i, d in enumerate(range(1,6)):\n    X = np.power.outer(H, np.arange(d+1))\n    M_CV = cross_validate(M, X, Y, cv=cv)\n    cv_error[i] = np.mean(M_CV['test_score'])\ncv_error\nprint(cv_error)\n\nKFold(n_splits=40, random_state=0, shuffle=True)\n[24.11505971 19.04928568 19.14454426 19.28592384 18.93589665]\n\n\n- 두 개로 쪼개서 validation error를 확인\n\nCV를 이렇게 숫자를 지정해서 하는 방법도 있다는 정도..\n\n\nvalidation = ShuffleSplit(n_splits=1, test_size=196, random_state=0)\nresults = cross_validate(hp_model,\nAuto.drop(['mpg'], axis=1), Auto['mpg'], cv =validation)\nprint(results)\n\n{'fit_time': array([0.00582528]), 'score_time': array([0.00158834]), 'test_score': array([23.61661707])}\n\n\n\n\n3. Bootstrap\n- \\(Var ( \\alpha X + ( 1- \\alpha)Y)\\)를 최소화하는 문제: 수식에 의해서 \\(\\alpha\\)에 대한 해는 \\[ \\frac{ \\sigma_Y^2 - \\sigma_{XY} }{  \\sigma_X^2 + \\sigma_Y^2 - 2 \\sigma_{XY} }\\]\n\nX가 \\(\\alpha\\) 만큼의 비중 Y가 \\(1-\\alpha\\)만큼의 비중\n분산을 줄이고싶음\n\n- 주어진 데이터의 인뎅싱을 통해서 공분산을 얻고 해를 구함\n\nPortfolio = load_data('Portfolio')\ndef alpha_func(D, idx):\n    cov_ = np.cov(D[['X','Y']].loc[idx], rowvar=False)\n    return ((cov_[1,1] - cov_[0,1]) / (cov_[0,0]+cov_[1,1]-2*cov_[0,1]))\n\n- 위의 함수 구동\n\nindex를 랜덤하게 정해서 결과를 확인\n\n\nrng = np.random.default_rng()\nidx = rng.choice(100, 100, replace=True)\nidx = range(Portfolio.shape[0])\nalpha_func(Portfolio, idx)\n\n0.5758320745928298\n\n\n- 붓스트랩 기법의 적용(\\(\\alpha\\))\n\n\\(\\alpha\\)에 대한 계산을 여러 개의 복원추출 샘플로 구함\n여기 나온 여래개의 \\(\\alpha\\)로 (추정량)\\(\\hat{\\alpha}\\)의 표준편차를 추정\n\n\ndef boot_SE(func, D, n=None, B=1000, seed=0):\n    rng = np.random.default_rng(seed)\n    first_ , second_ = 0, 0\n    n = n or D.shape[0]\n    for _ in range(B):\n        idx = rng.choice(D.index, n, replace=True)\n        value = func(D, idx)\n        first_ += value\n        second_ += value**2\n    return np.sqrt(second_ / B - (first_ / B)**2) # 표준편차 계산\n\nalpha_SE = boot_SE(alpha_func, Portfolio ,B=1000, seed=1)\nalpha_SE\n\n0.08945522198999856\n\n\n- 붓스트랩 기법의 적용(OLS)\n\n복원추출로 여러 개의 샘플을 만들어 회귀계수들을 확인\n함수를 구동할 때 모든 인자가 아니라 필요한 인자만 받을 수 있게 함\n앞에 두 인자는 고정시키게 됨\n\n\ndef boot_OLS(model_matrix, response, D, idx):\n   D_ = D.iloc[idx]     #D.iloc #\n   Y_ = D_[response]\n   X_ = clone(model_matrix).fit_transform(D_)\n   return sm.OLS(Y_, X_).fit().params\n\nhp_func = partial(boot_OLS, MS(['horsepower']), 'mpg')\n\n\nrng = np.random.default_rng(0)\nkk = [hp_func(Auto, rng.choice(392, 392, replace=True)) for _ in range(10)]\nprint(np.array(kk))\n\n[[39.88064456 -0.1567849 ]\n [38.73298691 -0.14699495]\n [38.31734657 -0.14442683]\n [39.91446826 -0.15782234]\n [39.43349349 -0.15072702]\n [40.36629857 -0.15912217]\n [39.62334517 -0.15449117]\n [39.0580588  -0.14952908]\n [38.66688437 -0.14521037]\n [39.64280792 -0.15555698]]\n\n\n- 붓스트랩으로 얻은 표준오차와 일반 방법론으로 얻은 표준오차 비교\n\ndef boot_OLS(model_matrix, response, D, idx):\n   D_ = D.loc[idx]     #D.iloc #\n   Y_ = D_[response]\n   X_ = clone(model_matrix).fit_transform(D_)\n   return sm.OLS(Y_, X_).fit().params\n\nhp_func = partial(boot_OLS, MS(['horsepower']), 'mpg')\nhp_se = boot_SE(hp_func, Auto, B=1000, seed=0)\nprint(hp_se)\n\nhp_model.fit(Auto, Auto['mpg'])\nmodel_se = summarize(hp_model.results_)['std err']\nprint(summarize(M.fit())['std err'])\n\nintercept     0.717654\nhorsepower    0.006041\ndtype: float64\nintercept     0.717\nhorsepower    0.006\nName: std err, dtype: float64\n\n\n\nquad_model = MS([poly('horsepower', 2, raw=True)])\nquad_func = partial(boot_OLS, quad_model, 'mpg')\nprint(boot_SE(quad_func, Auto, B=1000))\n\nM = sm.OLS(Auto['mpg'], quad_model.fit_transform(Auto))\nprint(summarize(M.fit())['std err'])\n\nintercept                                  1.538641\npoly(horsepower, degree=2, raw=True)[0]    0.024696\npoly(horsepower, degree=2, raw=True)[1]    0.000090\ndtype: float64\nintercept                                  1.800\npoly(horsepower, degree=2, raw=True)[0]    0.031\npoly(horsepower, degree=2, raw=True)[1]    0.000\nName: std err, dtype: float64"
  },
  {
    "objectID": "posts/4.ML_HW1.html",
    "href": "posts/4.ML_HW1.html",
    "title": "4. ML HW1",
    "section": "",
    "text": "1. 캐글에서 설명변수의 개수가 5~8개이며 반응변수가 연속형인 데이터를 하나 선택하고 아래 정보를 제출하시오.\n\n1) 데이터 이름과 변수명, 데이터 크기\n- 데이터 이름 : Sleep Time Prediction\n\nimport pandas as pd\n\n\nSleep = pd.read_csv(\"sleeptime_prediction_dataset.csv\")\n\n- 변수명\n\nWorkoutTime : 활동에 사용한 시간 (hours/day)\nReadingTime : 독서에 사용한 시간 (hours/day)\nPhoneTime : 휴대폰 사용 시간 (hours/day)\nWorkHours : 일에 사용한 시간 (hours/day)\nCaffeineIntake : 카페인 소비량 (mg/day)\nRelaxationTime : 휴식에 사용한 시간 (hours/day)\nSleepTime : 수면에 사용한 총 시간 (hours/night)\n\n\nSleep.columns\n\nIndex(['WorkoutTime', 'ReadingTime', 'PhoneTime', 'WorkHours',\n       'CaffeineIntake', 'RelaxationTime', 'SleepTime'],\n      dtype='object')\n\n\n- 데이터 크기\n\nSleep.shape\n\n(2000, 7)\n\n\n\nprint(f\"행(데이터)의 수: {Sleep.shape[0]}\")\nprint(f\"열(변수)의 수: {Sleep.shape[1]}\")\n\n행(데이터)의 수: 2000\n열(변수)의 수: 7\n\n\n\n\n2) 각 변수별 기초 통계량(평균과 분산, 범주형인 경우 범주별 비율)\n- 모든 변수에 타이빙 연속형이고, null 값이 없는 것을 확인\n\nSleep.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2000 entries, 0 to 1999\nData columns (total 7 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   WorkoutTime     2000 non-null   float64\n 1   ReadingTime     2000 non-null   float64\n 2   PhoneTime       2000 non-null   float64\n 3   WorkHours       2000 non-null   float64\n 4   CaffeineIntake  2000 non-null   float64\n 5   RelaxationTime  2000 non-null   float64\n 6   SleepTime       2000 non-null   float64\ndtypes: float64(7)\nmemory usage: 109.5 KB\n\n\n- 각 변수의 평균과 분산 출력\n\nsummary_stats = pd.DataFrame({\n    'Mean': Sleep.mean(),\n    'Variance': Sleep.var()\n})\n\nprint(summary_stats)\n\n                      Mean     Variance\nWorkoutTime       1.495915     0.768497\nReadingTime       0.992785     0.333278\nPhoneTime         2.985195     1.326588\nWorkHours         6.926945     2.971634\nCaffeineIntake  147.493780  7165.815344\nRelaxationTime    1.010955     0.339445\nSleepTime         4.884375     4.116403\n\n\n- 추가적인 기초 통계량\n\nSleep.describe()\n\n\n\n\n\n\n\n\nWorkoutTime\nReadingTime\nPhoneTime\nWorkHours\nCaffeineIntake\nRelaxationTime\nSleepTime\n\n\n\n\ncount\n2000.000000\n2000.000000\n2000.000000\n2000.000000\n2000.000000\n2000.000000\n2000.000000\n\n\nmean\n1.495915\n0.992785\n2.985195\n6.926945\n147.493780\n1.010955\n4.884375\n\n\nstd\n0.876639\n0.577303\n1.151776\n1.723843\n84.651139\n0.582619\n2.028892\n\n\nmin\n0.010000\n0.000000\n1.000000\n4.000000\n0.020000\n0.000000\n0.150000\n\n\n25%\n0.710000\n0.500000\n1.990000\n5.440000\n75.980000\n0.510000\n3.840000\n\n\n50%\n1.520000\n0.990000\n2.965000\n6.910000\n146.010000\n1.010000\n4.600000\n\n\n75%\n2.250000\n1.500000\n3.960000\n8.422500\n218.902500\n1.530000\n5.470000\n\n\nmax\n3.000000\n2.000000\n5.000000\n10.000000\n299.850000\n2.000000\n19.810000\n\n\n\n\n\n\n\n\n\n3) 반응변수와 각 변수들 간의 산점도(행렬 형태 추천)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 설명변수들\nfeatures = ['WorkoutTime', 'ReadingTime', 'PhoneTime', 'WorkHours',\n            'CaffeineIntake', 'RelaxationTime']\n\n# 서브플롯 설정\nfig, axes = plt.subplots(2, 3, figsize=(15, 8)) \naxes = axes.flatten() \n\n# 각 변수에 대해 산점도 그리기\nfor i, feature in enumerate(features):\n    sns.scatterplot(x=Sleep[feature], y=Sleep['SleepTime'], ax=axes[i])\n    axes[i].set_title(f'SleepTime vs {feature}')\n\n# 여백 자동 조정\nplt.tight_layout()\nplt.suptitle(\"SleepTime과 각 변수 간 산점도 (2x3)\", y=1.03)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2. 다음을 시행하시오 (모두 다 파이썬이나 R 이용)\n\n1) 모든 변수를 이용하여 회귀분석을 시행하고 모형의 유의성을 유의수준 0.05에서 검정하시오\n\nimport statsmodels.api as sm\n\n- 독립변수(X)와 종속변수(y) 로 분리\n\nX = Sleep.drop(columns=['SleepTime'])\ny = Sleep['SleepTime']\n\n- 절편을 위한 상수항 추가\n\nX = sm.add_constant(X)\n\n- 상수항 이름 const에서 intercept로 변경\n\nX.columns= ['intercept','WorkoutTime', 'ReadingTime', 'PhoneTime', 'WorkHours',\n       'CaffeineIntake', 'RelaxationTime']\n\n\nprint(X[:5])\n\n   intercept  WorkoutTime  ReadingTime  PhoneTime  WorkHours  CaffeineIntake  \\\n0        1.0         1.12         0.52       3.29       7.89          216.08   \n1        1.0         2.85         0.49       4.22       5.03          206.18   \n2        1.0         2.20         1.81       4.04       9.23           28.73   \n3        1.0         1.80         0.50       1.62       7.68          276.77   \n4        1.0         0.47         0.54       1.60       4.94          170.54   \n\n   RelaxationTime  \n0            0.75  \n1            0.67  \n2            0.35  \n3            1.21  \n4            0.95  \n\n\n- F-statistic : 전체 회귀모형이 통계적으로 유의한지 검정하는 통계량\n\nProb (F-statistic) : F-검정의 p-value\n이 값이 (2.71e-127) 매우 작으므로 전체 모형이 유의 하다고 할 수 있다.\n\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              SleepTime   R-squared:                       0.261\nModel:                            OLS   Adj. R-squared:                  0.259\nMethod:                 Least Squares   F-statistic:                     117.5\nDate:                Tue, 15 Apr 2025   Prob (F-statistic):          2.71e-127\nTime:                        11:13:58   Log-Likelihood:                -3949.4\nNo. Observations:                2000   AIC:                             7913.\nDf Residuals:                    1993   BIC:                             7952.\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nintercept          7.8462      0.231     33.973      0.000       7.393       8.299\nWorkoutTime        0.4768      0.045     10.672      0.000       0.389       0.564\nReadingTime        0.2102      0.068      3.104      0.002       0.077       0.343\nPhoneTime         -0.5612      0.034    -16.520      0.000      -0.628      -0.495\nWorkHours         -0.3601      0.023    -15.857      0.000      -0.405      -0.316\nCaffeineIntake    -0.0014      0.000     -2.976      0.003      -0.002      -0.000\nRelaxationTime     0.4837      0.067      7.193      0.000       0.352       0.616\n==============================================================================\nOmnibus:                     2315.894   Durbin-Watson:                   2.023\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           157684.352\nSkew:                           6.152   Prob(JB):                         0.00\nKurtosis:                      44.723   Cond. No.                     1.02e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.02e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\n2) \\(\\hat{\\sigma^2} = \\sum_{i=1}^{n}\\frac{(\\hat{y}_i-y_i)^2}{n-p-1}\\)를 구하시오. 모든 변수들과 절편을 포함한 회귀분석에서의 예측을 이용하며 \\(p\\)는 사용한 모든 변수들의 개수임.\n- 예측값\n\ny_pred = results.fittedvalues\n\n- n, p 설정\n\nn = len(y)\np = X.shape[1]-1\n\n- \\(\\hat{\\sigma^2}\\) 계산\n\nsigma_squared_hat = np.sum(results.resid**2 / (n-p-1))\n\n- \\(\\hat{\\sigma^2}\\) = 3.0497\n\nprint(f'추정된 sigma^2 : {sigma_squared_hat:.4f}')\n\n추정된 sigma^2 : 3.0497\n\n\n\n\n3) 하나의 변수나 두 개의 변수를 사용하여 각 모형에서 \\(C_p=RSS/\\hat{\\sigma^2}+2p^*\\)를 계산하고 결과를 산점도로 표시하시오(가로축은 사용한 변수의 개수, 세로축은 계산된 \\(C_p^*\\)값). 여기에서 RSS는 회귀모형의 잔차곱합이며 \\(p^*\\)는 RSS에서 사용한 변수들의 개수이며, 하나의 변수를 사용하면 \\(p\\)개의 모형이 두 개의 변수를 사용하면 \\(p(p-1)\\)개의 모형이 고려될 수 있음(산점도에 총 \\(p+p(p-1))\\)개의 점이 찍혀야함)\n- 변수가 두개 일때는 p(p-1)/2 개의 점이 찍혀야하는것 아닌가요?..(중복제거)\n\nimport itertools\nimport matplotlib.pyplot as plt\n\n\ncp_list = []\npstar_list = []\nused_features = []\n\n# === 변수 1개씩 사용하는 모형들 ===\nfor var in features:\n    used_features.append(var)\n    X = sm.add_constant(Sleep[[var]])  # 절편 포함\n    model = sm.OLS(y, X).fit()\n    rss = sum((y-model.fittedvalues)**2)\n    p_star = 2  # 변수 1개 + 절편\n    cp = rss / sigma_squared_hat + 2 * p_star\n    cp_list.append(cp)\n    pstar_list.append(p_star)\n\n# === 변수 2개씩 사용하는 모형들 ===\nfor var1, var2 in itertools.combinations(features, 2):\n    used_features.append([var1,var2])\n    X = sm.add_constant(Sleep[[var1, var2]])\n    model = sm.OLS(y, X).fit()\n    rss = sum((model.fittedvalues - y)**2)\n    p_star = 3  # 변수 2개 + 절편\n    cp = rss / sigma_squared_hat + 2 * p_star\n    cp_list.append(cp)\n    pstar_list.append(p_star)\n\n\n# === 산점도 그리기 ===\nplt.figure(figsize=(8, 5))\nplt.scatter(pstar_list, cp_list, alpha=0.7)\nplt.xlabel(\"사용한 변수 개수 (p*)\")\nplt.ylabel(\"C_p 값\")\nplt.title(\"$C_p$ vs 변수 개수 산점도\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4) 계산한 3)의 값들 중 가장 작은 값은 어떤 변수(들)을 사용할 때인가?\n- PhoneTime, WorkHours 를 사용할 때 \\(C_p\\)의 값이 가장 작다\n\nused_features[np.argmin(cp_list)]\n\n['PhoneTime', 'WorkHours']\n\n\n\n\n\n3. 반응변수가 중앙값을 초과할 때 1, 이하일 때 0으로 하여 아래를 시행하시오.\n\n1) 로지스틱 회귀분석을 시행해서 혼동행렬을 구하시오\n\nfrom ISLP import confusion_table\nfrom ISLP.models import (ModelSpec as MS,summarize)\nfrom sklearn.model_selection import train_test_split\n\n- y 를 반응변수 SleepTime의 중앙값을 초과하면 1, 이하이면 0\n\nX = Sleep.drop(columns=['SleepTime'])\ny = (Sleep['SleepTime']&gt;median_sleep).astype(int)\n\n- 상수항 추가\n\ndesign = MS(X)\n\n\nX=design.fit_transform(X)\n\n\nX[:5]\n\n\n\n\n\n\n\n\nintercept\nWorkoutTime\nReadingTime\nPhoneTime\nWorkHours\nCaffeineIntake\nRelaxationTime\n\n\n\n\n0\n1.0\n1.12\n0.52\n3.29\n7.89\n216.08\n0.75\n\n\n1\n1.0\n2.85\n0.49\n4.22\n5.03\n206.18\n0.67\n\n\n2\n1.0\n2.20\n1.81\n4.04\n9.23\n28.73\n0.35\n\n\n3\n1.0\n1.80\n0.50\n1.62\n7.68\n276.77\n1.21\n\n\n4\n1.0\n0.47\n0.54\n1.60\n4.94\n170.54\n0.95\n\n\n\n\n\n\n\n- train과 test로 분리 비율 0.7 : 0.3\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n- 로지스틱 회귀분석 모델 생성 및 적합\n\nglm = sm.GLM(y_train, X_train, family=sm.families.Binomial())\nresults = glm.fit()\nprint(summarize(results))\n\n                  coef  std err       z  P&gt;|z|\nWorkoutTime     1.6071    0.105  15.348  0.000\nReadingTime     1.5103    0.139  10.838  0.000\nPhoneTime      -1.0310    0.072 -14.263  0.000\nWorkHours      -0.4311    0.036 -11.868  0.000\nCaffeineIntake  0.0004    0.001   0.487  0.626\nRelaxationTime  1.7463    0.141  12.413  0.000\n\n\n- 예측한 확률이 0.5보다 크면 1로, 작으면 0으로 labels에 저장\n\nprobs = results.predict(exog=X_test)\nlabels = np.array([0]*len(y_test))  # 기본값 0\nlabels[probs &gt; 0.5] = 1 \nprint(list(probs)[:5])\nprint(labels[:5])\n\n[0.9911024037508189, 0.13970493273847814, 0.9362503403933953, 0.9184685442097736, 0.012038711545818204]\n[1 0 1 1 0]\n\n\n- 혼동행렬\n\nconfusion_table(labels, y_test)\n\n\n\n\n\n\n\nTruth\n0\n1\n\n\nPredicted\n\n\n\n\n\n\n0\n248\n68\n\n\n1\n42\n242\n\n\n\n\n\n\n\n- 정확도\n\nnp.mean(labels == y_test)\n\n0.8166666666666667\n\n\n\n\n2) LDA를 시행해서 혼동행렬을 구하시오\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\n- 상수항이 없는 X데이터 사용\n\nX = Sleep.drop(columns=['SleepTime'])\ny = (Sleep['SleepTime']&gt;median_sleep).astype(int)\n\n- train과 test로 분리 비율 0.7:0.3\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n- LDA 적합 및 예측\n\nlda = LDA(store_covariance=True)\n\nlda.fit(X_train, y_train)\n\nlda_pred = lda.predict(X_test)\n\nprint(lda_pred[:15])\n\n[1 0 1 1 0 0 0 1 1 0 0 1 0 0 1]\n\n\n- 혼동행렬\n\nconfusion_table(lda_pred, y_test)\n\n\n\n\n\n\n\nTruth\n0\n1\n\n\nPredicted\n\n\n\n\n\n\n0\n288\n15\n\n\n1\n2\n295\n\n\n\n\n\n\n\n\n\n3) 2)번에서 민감도와 특이도는 어떻게 되는가. 민감도는 1을 기준으로 함\n- 민감도 TP/(TP+FN)\n- 특이도 TN/(TN+FP)\n\nTP : 실제1 \\(\\to\\) 예측1 (정답)\nFN : 실제1 \\(\\to\\) 예측0 (틀림)\nTN : 실제0 \\(\\to\\) 예측0 (정답)\nFP : 실제0 \\(\\to\\) 예측1 (틀림)\n\n- 혼동행렬\n\ncm = confusion_table(lda_pred,y_test)\n\n\ncm\n\n\n\n\n\n\n\nTruth\n0\n1\n\n\nPredicted\n\n\n\n\n\n\n0\n288\n15\n\n\n1\n2\n295\n\n\n\n\n\n\n\n- 2차원구조 인덱싱을 통해 값 대입\n\nTP = cm[1][1]\nFN = cm[1][0]\nTN = cm[0][0]\nFP = cm[0][1]\n\n\nTP, FN, TN, FP\n\n(295, 15, 288, 2)\n\n\n- 민감도 특이도 계산\n\n# 민감도 (class 1 기준)\nsensitivity = TP / (TP + FN)\n\n# 특이도 (class 0 기준)\nspecificity = TN / (TN + FP)\n\n\nprint(f\"민감도 (class=1): {sensitivity:.2f}\")\nprint(f\"특이도 (class=0): {specificity:.2f}\")\n\n민감도 (class=1): 0.95\n특이도 (class=0): 0.99\n\n\n\n\n4) 3)번에서 예측의 규칙을 바꾸어 1에 대한 예측확률이 0.20 이상일 때 1로 예측하면 민감도와 특이도는 어떻게 변하는가?\n- 1 값에 대한 확률 계산\n\nprobs[:5]\n\narray([0.99768879, 0.0077565 , 0.96238128, 0.99887321, 0.0017131 ])\n\n\n\nprobs = lda.predict_proba(X_test)[:,1] \n\n- 예측기준 0.20으로 설정해 새로운 예측값 생성\n\ncustom_pred = (probs &gt; 0.20).astype(int)\n\n\ncm = confusion_table(custom_pred,y_test)\n\n- 새로운 혼동행렬\n\ncm\n\n\n\n\n\n\n\nTruth\n0\n1\n\n\nPredicted\n\n\n\n\n\n\n0\n211\n8\n\n\n1\n79\n302\n\n\n\n\n\n\n\n\nsum(custom_pred ==1)\n\n381\n\n\n\nTP = cm[1][1]\nFN = cm[1][0]\nTN = cm[0][0]\nFP = cm[0][1]\n\n\nTP, FN, TN, FP\n\n(302, 8, 211, 79)\n\n\n- 민감도 특이도 계산\n\n# 민감도 (class 1 기준)\nsensitivity = TP / (TP + FN)\n\n# 특이도 (class 0 기준)\nspecificity = TN / (TN + FP)\n\n\nprint(f\"민감도 (class=1): {sensitivity:.2f}\")\nprint(f\"특이도 (class=0): {specificity:.2f}\")\n\n민감도 (class=1): 0.97\n특이도 (class=0): 0.73\n\n\n- 클래스 1로 분류되는 사례 TP, FP가 늘어나면서 민감도가 증가하고 특이도가 감소하였다"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML2025",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 29, 2025\n\n\n6. Linear Model Selection and Regularization\n\n\n이상민 \n\n\n\n\nApr 15, 2025\n\n\n5. Sampling & Resampling\n\n\n이상민 \n\n\n\n\nApr 14, 2025\n\n\n4. ML HW1\n\n\n이상민 \n\n\n\n\nApr 8, 2025\n\n\n3. Classification\n\n\n이상민 \n\n\n\n\nApr 5, 2025\n\n\n2. Linear Regression\n\n\n이상민 \n\n\n\n\nApr 4, 2025\n\n\n1. Basics\n\n\n이상민 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/3.Classification.html",
    "href": "posts/3.Classification.html",
    "title": "3. Classification",
    "section": "",
    "text": "1. imports\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.pyplot import subplots\nimport statsmodels.api as sm\nfrom ISLP import load_data\nfrom ISLP.models import (ModelSpec as MS,summarize)\n\n\nfrom ISLP import confusion_table\nfrom ISLP.models import contrast\nfrom sklearn.discriminant_analysis import  (LinearDiscriminantAnalysis as LDA, QuadraticDiscriminantAnalysis as QDA)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\n\n\n\n2. Smarket Data 분석\n- 불러오기\n\nSmarket = load_data('Smarket')\nSmarket\nprint(Smarket.columns)\n\nimport copy\nSmar = copy.deepcopy(Smarket)\n\nIndex(['Year', 'Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5', 'Volume', 'Today',\n       'Direction'],\n      dtype='object')\n\n\n\nset(Smar.Direction)\n\n{'Down', 'Up'}\n\n\n- Direction 을 인코딩(수치화) 후 상관행렬 계산\n\nSmar['Direction'] = Smar['Direction'].map({'Up': 1, 'Down': 0})\nprint(Smar.corr())\n\n               Year      Lag1      Lag2      Lag3      Lag4      Lag5  \\\nYear       1.000000  0.029700  0.030596  0.033195  0.035689  0.029788   \nLag1       0.029700  1.000000 -0.026294 -0.010803 -0.002986 -0.005675   \nLag2       0.030596 -0.026294  1.000000 -0.025897 -0.010854 -0.003558   \nLag3       0.033195 -0.010803 -0.025897  1.000000 -0.024051 -0.018808   \nLag4       0.035689 -0.002986 -0.010854 -0.024051  1.000000 -0.027084   \nLag5       0.029788 -0.005675 -0.003558 -0.018808 -0.027084  1.000000   \nVolume     0.539006  0.040910 -0.043383 -0.041824 -0.048414 -0.022002   \nToday      0.030095 -0.026155 -0.010250 -0.002448 -0.006900 -0.034860   \nDirection  0.074608 -0.039757 -0.024081  0.006132  0.004215  0.005423   \n\n             Volume     Today  Direction  \nYear       0.539006  0.030095   0.074608  \nLag1       0.040910 -0.026155  -0.039757  \nLag2      -0.043383 -0.010250  -0.024081  \nLag3      -0.041824 -0.002448   0.006132  \nLag4      -0.048414 -0.006900   0.004215  \nLag5      -0.022002 -0.034860   0.005423  \nVolume     1.000000  0.014592   0.022951  \nToday      0.014592  1.000000   0.730563  \nDirection  0.022951  0.730563   1.000000  \n\n\n- 변수중 하나인 Volume을 시각화\n\nplt.plot(Smar['Volume'])\nplt.show()\n\n\n\n\n\n\n\n\n- 반응변수와 Today, Year 변수를 제외\n\nMS : 데이터 전처리를 위한 함수\n문자로 되어있는 Direction변수를 0,1로 변환\n로지스틱 회귀분석 적용\n\n\nallvars = Smarket .columns.drop(['Today', 'Direction', 'Year'])\ndesign = MS(allvars)\nX = design.fit_transform(Smarket)\ny = Smarket.Direction == \"Up\"\nprint(y)\nglm = sm.GLM(y, X, family=sm.families.Binomial())\nresults = glm.fit()\nprint(summarize(results))\n\n0        True\n1        True\n2       False\n3        True\n4        True\n        ...  \n1245     True\n1246    False\n1247     True\n1248    False\n1249    False\nName: Direction, Length: 1250, dtype: bool\n             coef  std err      z  P&gt;|z|\nintercept -0.1260    0.241 -0.523  0.601\nLag1      -0.0731    0.050 -1.457  0.145\nLag2      -0.0423    0.050 -0.845  0.398\nLag3       0.0111    0.050  0.222  0.824\nLag4       0.0094    0.050  0.187  0.851\nLag5       0.0103    0.050  0.208  0.835\nVolume     0.1354    0.158  0.855  0.392\n\n\n- 예측된 확률\n\nlabel 을 Down으로 채운 뒤 만약 예측된 확률 &gt; 0.5 이면 Up으로 예측(변경)\n훈련 데이터의 예측에 대한 혼동행렬 계산\n예측정확도 계산\n\n\nprobs = results.predict()\nprint(probs[:10])\nlabels = np.array(['Down']*1250)\nlabels[probs&gt;0.5] = \"Up\"\nprint(confusion_table(labels, Smarket.Direction))\nnp.mean(labels == Smarket.Direction)\n\n[0.50708413 0.48146788 0.48113883 0.51522236 0.51078116 0.50695646\n 0.49265087 0.50922916 0.51761353 0.48883778]\nTruth      Down   Up\nPredicted           \nDown        145  141\nUp          457  507\n\n\n0.5216\n\n\n- 훈련 데이터를 2005년 전과 후로 쪼갬\n\n훈련 데이터로 학습한 모형으로부터 평가데이터의 예측확률 저장\n\\(\\to\\) 이를 통해 혼동행렬과 예측 정확도 계산\n\n\ntrain = (Smarket.Year &lt; 2005)\nSmarket_train = Smarket.loc[train]\nSmarket_test = Smarket.loc[~train]\nSmarket_test.shape\n\n(252, 9)\n\n\n\nX_train, X_test = X.loc[train], X.loc[~train]\ny_train, y_test = y.loc[train], y.loc[~train]\nglm_train = sm.GLM(y_train, X_train,family=sm.families.Binomial())\nresults = glm_train.fit()\nprobs = results.predict(exog=X_test)\n\n\nD = Smarket.Direction\nL_train, L_test = D.loc[train], D.loc[~train]\nlabels = np.array(['Down']*252)\nlabels[probs&gt;0.5] = 'Up'\nprint(confusion_table(labels, L_test))\nnp.mean(labels != L_test)\n\nTruth      Down  Up\nPredicted          \nDown         77  97\nUp           34  44\n\n\n0.5198412698412699\n\n\n- 전체 변수가 아닌 Lag1과 Lag2만으로 로지스틱 회귀분석 진행\n\n혼동행렬을 통해서 예측정확도 파악\n정확도가 더 낮은 것을 알 수 있음\n\n\nmodel = MS(['Lag1', 'Lag2']).fit(Smarket)\nX = model.transform(Smarket)\nX_train, X_test = X.loc[train], X.loc[~train]\nglm_train = sm.GLM(y_train,\nX_train ,\nfamily=sm.families.Binomial())\nresults = glm_train.fit()\nprobs = results.predict(exog=X_test)\nlabels = np.array(['Down']*252)\nlabels[probs&gt;0.5] = 'Up'\nprint(confusion_table(labels, L_test))\nnp.mean(labels !=L_test)\n\nTruth      Down   Up\nPredicted           \nDown         35   35\nUp           76  106\n\n\n0.44047619047619047\n\n\n- 새로운 데이터가 들어왔을 때, 예측확률 계산\n\n모두 0.5보다 작으므로 0 \\(\\to\\) Down으로 예측\n\n\nnewdata = pd.DataFrame({'Lag1':[1.2, 1.5], 'Lag2':[1.1, -0.8]});\nnewX = model.transform(newdata)\nprint(results.predict(newX))\n\n0    0.479146\n1    0.496094\ndtype: float64\n\n\n\n\n3. LDA & QDA\n- 앞의 데이터 이용\n\nLDA로 예측변수들이 주어졌을 때, 0과 1의 확률을 계산\n0과 1의 종류별로 평균벡터와 공유하는 공분산행렬 그리고 각 종에 대한 prior를 보여줌\n여기서 prior은 1실제 0과 1의 비율\n\n- LDA\n\n각 클래스의 공분산이 같다고 가정\n공분산행렬 하나만 나옴\n직접 계산한 클래스의 비율과 자동으로 계산된 비율 같은거!\nprior을 지정할 수도 있음\n\n\nX_train[:5]\n\n\n\n\n\n\n\n\nintercept\nLag1\nLag2\n\n\n\n\n0\n1.0\n0.381\n-0.192\n\n\n1\n1.0\n0.959\n0.381\n\n\n2\n1.0\n1.032\n0.959\n\n\n3\n1.0\n-0.623\n1.032\n\n\n4\n1.0\n0.614\n-0.623\n\n\n\n\n\n\n\n\n#store_covariance=True : 공분산행렬 저장\nlda = LDA(store_covariance=True)\n#상수항 intercept 제거\nXX_train, XX_test = [M.drop(columns=['intercept']) for M in [X_train, X_test]]\n\na, c = np.unique(L_train, return_counts=True)\nprint(f'각각의 라벨의 비율 : \\n{c/np.sum(c)}')\n\nlda.fit(XX_train, L_train)\nprint(f'각 클래스의 평균벡터 : \\n         Lag1       Lag2\\nDown - {lda.means_[0]}\\nUp   - {lda.means_[1]}')\nprint(f'공분산 행렬 : \\n{lda.covariance_}')\nprint(f'클래스의 종류 : \\n{lda.classes_}')\nprint(f'각 클래스의 비율 : \\n{lda.priors_}')\n\n각각의 라벨의 비율 : \n[0.49198397 0.50801603]\n각 클래스의 평균벡터 : \n         Lag1       Lag2\nDown - [0.04279022 0.03389409]\nUp   - [-0.03954635 -0.03132544]\n공분산 행렬 : \n[[ 1.50886781 -0.03340234]\n [-0.03340234  1.5095363 ]]\n클래스의 종류 : \n['Down' 'Up']\n각 클래스의 비율 : \n[0.49198397 0.50801603]\n\n\n- LDA를 이용한 예측 결과 및 혼동행렬\n\nlda_pred = lda.predict(XX_test)\nprint(lda_pred[:15])\nprint(confusion_table(lda_pred, L_test))\n\n['Up' 'Up' 'Up' 'Up' 'Up' 'Up' 'Up' 'Up' 'Up' 'Up' 'Up' 'Down' 'Up' 'Up'\n 'Up']\nTruth      Down   Up\nPredicted           \nDown         35   35\nUp           76  106\n\n\n\nlda_prob = lda.predict_proba(XX_test)\nprint(lda_prob[:15])\n\n[[0.49017925 0.50982075]\n [0.4792185  0.5207815 ]\n [0.46681848 0.53318152]\n [0.47400107 0.52599893]\n [0.49278766 0.50721234]\n [0.49385615 0.50614385]\n [0.49510156 0.50489844]\n [0.4872861  0.5127139 ]\n [0.49070135 0.50929865]\n [0.48440262 0.51559738]\n [0.49069628 0.50930372]\n [0.51199885 0.48800115]\n [0.48951523 0.51048477]\n [0.47067612 0.52932388]\n [0.47445929 0.52554071]]\n\n\n- QDA를 이용한 결과\n\n여기서는 공분산 행렬이 공유되지 않고 0과 1별로 별도의 공분산 행렬 추정됨\n\n\nqda = QDA(store_covariance=True)\nprint(qda.fit(XX_train, L_train))\n\nqda.means_, qda.priors_\n\nprint(f'0의 공분산 행렬 : \\n{qda.covariance_[0]}')\nprint(f'1의 공분산 행렬 : \\n{qda.covariance_[1]}')\n\nQuadraticDiscriminantAnalysis(store_covariance=True)\n0의 공분산 행렬 : \n[[ 1.50662277 -0.03924806]\n [-0.03924806  1.53559498]]\n1의 공분산 행렬 : \n[[ 1.51700576 -0.02787349]\n [-0.02787349  1.49026815]]\n\n\n- QDA로 예측한 결과 및 혼합행렬 + 예측 정확도\n\nqda_pred = qda.predict(XX_test)\n#print(qda_pred)\nprint(confusion_table(qda_pred, L_test))\nnp.mean(qda_pred == L_test)\n\nTruth      Down   Up\nPredicted           \nDown         30   20\nUp           81  121\n\n\n0.5992063492063492\n\n\n\n\n4. Naive Bayes\n\nNaive 베이즈를 이용한 결과\n여기에서 theta는 각 종별 평균벡터 var_는 공분산 행렬의 대각값들을 의미(각 종별 분산)\n\n\nsum(L_train=='Up')/len(L_train)\n\n0.5080160320641283\n\n\n\nNB = GaussianNB()\nrs = NB.fit(XX_train, L_train)\nprint(set(L_train))\nprint(rs)\nprint(f'Down,Up의 비율 : \\n{NB.class_prior_}')\nprint(f'각 종별 평균 벡터 : \\n{NB.theta_}')\nprint(f'공분산 행렬의 대각값(각 종별 분산) : \\n{NB.var_}')\n\n{'Up', 'Down'}\nGaussianNB()\nDown,Up의 비율 : \n[0.49198397 0.50801603]\n각 종별 평균 벡터 : \n[[ 0.04279022  0.03389409]\n [-0.03954635 -0.03132544]]\n공분산 행렬의 대각값(각 종별 분산) : \n[[1.50355429 1.53246749]\n [1.51401364 1.48732877]]\n\n\n- Naive 베이지를 이용한 예측\n\n혼합행렬의 결과\nUp은 괜찮게 맞추는데 Down은 거의 맞추지 못함\n\n\nnb_labels = NB.predict(XX_test)\nprint(confusion_table(nb_labels , L_test))\n\nTruth      Down   Up\nPredicted           \nDown         29   20\nUp           82  121\n\n\n\n\n5. KNN classifier\n- 위의 데이터에 적용\n\nknn1 = KNeighborsClassifier(n_neighbors=1)\nknn1.fit(XX_train , L_train)\nknn1_pred = knn1.predict(XX_test)\nprint(\"최근접 이웃 1개\")\nprint(confusion_table(knn1_pred , L_test))\nknn3 = KNeighborsClassifier(n_neighbors=3)\nknn3_pred = knn3.fit(XX_train , L_train).predict(XX_test)\nprint(\"최근접 이웃 3개\")\nprint(confusion_table(knn3_pred , L_test))\n\n최근접 이웃 1개\nTruth      Down  Up\nPredicted          \nDown         43  58\nUp           68  83\n최근접 이웃 3개\nTruth      Down  Up\nPredicted          \nDown         48  55\nUp           63  86\n\n\n- 데이터를 불러오기\n\nNo 가 훨씬 많음\n\n\n# data loading\n# ------------\nCaravan = load_data('Caravan')\nPurchase = Caravan.Purchase\nprint(Purchase.value_counts())\nfeature_df = Caravan.drop(columns=['Purchase'])\n\nPurchase\nNo     5474\nYes     348\nName: count, dtype: int64\n\n\n- 표준화\n\nwith_mean=True : 평균 1\nwith_std=True : 표준편차 1\n표준화를 해서 표준화된 값을 X_std에 저장 후\n데이터프레임으로 변환\n결과 : 모두 표준편차가 1에 가깝게 표준화 됨\n\n\n\nscaler = StandardScaler(with_mean=True, with_std=True, copy=True)\nscaler.fit(feature_df)\nX_std = scaler.transform(feature_df)\nfeature_std = pd.DataFrame(X_std , columns=feature_df.columns)\nfeature_std.std()\n\nMOSTYPE     1.000086\nMAANTHUI    1.000086\nMGEMOMV     1.000086\nMGEMLEEF    1.000086\nMOSHOOFD    1.000086\n              ...   \nAZEILPL     1.000086\nAPLEZIER    1.000086\nAFIETS      1.000086\nAINBOED     1.000086\nABYSTAND    1.000086\nLength: 85, dtype: float64\n\n\n- KNN을 이용해서 분류를 하고, 테스트 데이터에서 성능을 확인\n- No는 잘 맞추는데 Yes는 못맞춤..\n\n(X_train, X_test, y_train, y_test) = train_test_split(feature_std, Purchase, test_size=1000, random_state=0)\nknn1 = KNeighborsClassifier(n_neighbors=1)\nknn1_pred = knn1.fit(X_train , y_train).predict(X_test)\n\nnp.mean(y_test != knn1_pred), np.mean(y_test != \"No\")\nprint(confusion_table(knn1_pred , y_test))\n\nTruth       No  Yes\nPredicted          \nNo         881   58\nYes         52    9\n\n\n- 이웃수를 증가시켜도 Yes에 대한 정확도가 증가하지 않음\n\nfor K in range(1,6):\n  knn = KNeighborsClassifier(n_neighbors=K)\n  knn_pred = knn.fit(X_train , y_train).predict(X_test)\n  C = confusion_table(knn_pred, y_test)\n  templ = 'K={0:d}: # predicted to rent: {1:&gt;2},' +' # who did rent {2:d}, accuracy {3:.1%}'\n  pred = C.loc['Yes'].sum()\n  did_rent = C.loc['Yes','Yes']\n  print(templ.format(K, pred, did_rent, did_rent / pred))\n  # rent를 한 사람을 맞춘 것에 대한 정확도임\n\nK=1: # predicted to rent: 61, # who did rent 9, accuracy 14.8%\nK=2: # predicted to rent:  6, # who did rent 1, accuracy 16.7%\nK=3: # predicted to rent: 19, # who did rent 3, accuracy 15.8%\nK=4: # predicted to rent:  3, # who did rent 0, accuracy 0.0%\nK=5: # predicted to rent:  7, # who did rent 1, accuracy 14.3%\n\n\n- 로지스틱 회귀분석과의 비교\n\nlogit = LogisticRegression(C=1e10 , solver='liblinear')\nlogit.fit(X_train , y_train)\nlogit_pred = logit.predict_proba(X_test)\nprint(\"말안되는 거, 확률이 5보다 클 수 없음 : Yes 0개\")\nlogit_labels = np.where(logit_pred[:,1] &gt; 5, 'Yes', 'No')\nprint(confusion_table(logit_labels , y_test))\nprint(\"확률 0.25 이상이면 Yes로 판단\")\nlogit_labels = np.where(logit_pred[:,1]&gt;0.25, 'Yes', 'No')\nprint(confusion_table(logit_labels , y_test))\n\n말안되는 거, 확률이 5보다 클 수 없음 : Yes 0개\nTruth       No  Yes\nPredicted          \nNo         933   67\nYes          0    0\n확률 0.25 이상이면 Yes로 판단\nTruth       No  Yes\nPredicted          \nNo         913   58\nYes         20    9\n\n\n\n\n6. 제약 및 다양한 회귀분석\n- 데이터 불러오기\n\n# data loading\n# ------------\nBike = load_data('Bikeshare')\nBike.shape, Bike.columns\n\n((8645, 15),\n Index(['season', 'mnth', 'day', 'hr', 'holiday', 'weekday', 'workingday',\n        'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'casual',\n        'registered', 'bikers'],\n       dtype='object'))\n\n\n- 일반 OLS\n\n선형 회귀 모델\n최소제곱법\n\n- 가변수 인코딩 방식 1)\n\n범주형 변수 인코딩필요\n하나의 범주를 기준으로 나머지 범주 0,1로 표현\n마지막 범주는 생략\n\n\nX = MS(['mnth', 'hr', 'workingday', 'temp', 'weathersit']).fit_transform(Bike)\nY = Bike['bikers']\nM_lm = sm.OLS(Y, X).fit()\nprint(summarize(M_lm))\n# 가변수의 경우 범주의 수 - 1개의 더미 변수를 구성\n\n                                 coef  std err       t  P&gt;|t|\nintercept                    -68.6317    5.307 -12.932  0.000\nmnth[Feb]                      6.8452    4.287   1.597  0.110\nmnth[March]                   16.5514    4.301   3.848  0.000\nmnth[April]                   41.4249    4.972   8.331  0.000\nmnth[May]                     72.5571    5.641  12.862  0.000\nmnth[June]                    67.8187    6.544  10.364  0.000\nmnth[July]                    45.3245    7.081   6.401  0.000\nmnth[Aug]                     53.2430    6.640   8.019  0.000\nmnth[Sept]                    66.6783    5.925  11.254  0.000\nmnth[Oct]                     75.8343    4.950  15.319  0.000\nmnth[Nov]                     60.3100    4.610  13.083  0.000\nmnth[Dec]                     46.4577    4.271  10.878  0.000\nhr[1]                        -14.5793    5.699  -2.558  0.011\nhr[2]                        -21.5791    5.733  -3.764  0.000\nhr[3]                        -31.1408    5.778  -5.389  0.000\nhr[4]                        -36.9075    5.802  -6.361  0.000\nhr[5]                        -24.1355    5.737  -4.207  0.000\nhr[6]                         20.5997    5.704   3.612  0.000\nhr[7]                        120.0931    5.693  21.095  0.000\nhr[8]                        223.6619    5.690  39.310  0.000\nhr[9]                        120.5819    5.693  21.182  0.000\nhr[10]                        83.8013    5.705  14.689  0.000\nhr[11]                       105.4234    5.722  18.424  0.000\nhr[12]                       137.2837    5.740  23.916  0.000\nhr[13]                       136.0359    5.760  23.617  0.000\nhr[14]                       126.6361    5.776  21.923  0.000\nhr[15]                       132.0865    5.780  22.852  0.000\nhr[16]                       178.5206    5.772  30.927  0.000\nhr[17]                       296.2670    5.749  51.537  0.000\nhr[18]                       269.4409    5.736  46.976  0.000\nhr[19]                       186.2558    5.714  32.596  0.000\nhr[20]                       125.5492    5.704  22.012  0.000\nhr[21]                        87.5537    5.693  15.378  0.000\nhr[22]                        59.1226    5.689  10.392  0.000\nhr[23]                        26.8376    5.688   4.719  0.000\nworkingday                     1.2696    1.784   0.711  0.477\ntemp                         157.2094   10.261  15.321  0.000\nweathersit[cloudy/misty]     -12.8903    1.964  -6.562  0.000\nweathersit[heavy rain/snow] -109.7446   76.667  -1.431  0.152\nweathersit[light rain/snow]  -66.4944    2.965 -22.425  0.000\n\n\n- 가변수 인코딩 방식 2) 합 제약\n\n모든 계수들의 합이 0이 되도록\n마지막 계수는 나머지 계수들의 합의 음의 값\n\n\nhr_encode = contrast('hr', 'sum')     # 합에 대한 제약을 검\nmnth_encode = contrast('mnth', 'sum') # 합에 대한 제약을 검\nX2 = MS([mnth_encode, hr_encode, 'workingday','temp', 'weathersit']).fit_transform(Bike)\nM2_lm = sm.OLS(Y, X2).fit()\nS2 = summarize(M2_lm)\nprint(S2)\n# 마지막 범주의 계수는 나머지 계수들의 합에 음수를 취함\n\n                                 coef  std err       t  P&gt;|t|\nintercept                     73.5974    5.132  14.340  0.000\nmnth[Jan]                    -46.0871    4.085 -11.281  0.000\nmnth[Feb]                    -39.2419    3.539 -11.088  0.000\nmnth[March]                  -29.5357    3.155  -9.361  0.000\nmnth[April]                   -4.6622    2.741  -1.701  0.089\nmnth[May]                     26.4700    2.851   9.285  0.000\nmnth[June]                    21.7317    3.465   6.272  0.000\nmnth[July]                    -0.7626    3.908  -0.195  0.845\nmnth[Aug]                      7.1560    3.535   2.024  0.043\nmnth[Sept]                    20.5912    3.046   6.761  0.000\nmnth[Oct]                     29.7472    2.700  11.019  0.000\nmnth[Nov]                     14.2229    2.860   4.972  0.000\nhr[0]                        -96.1420    3.955 -24.307  0.000\nhr[1]                       -110.7213    3.966 -27.916  0.000\nhr[2]                       -117.7212    4.016 -29.310  0.000\nhr[3]                       -127.2828    4.081 -31.191  0.000\nhr[4]                       -133.0495    4.117 -32.319  0.000\nhr[5]                       -120.2775    4.037 -29.794  0.000\nhr[6]                        -75.5424    3.992 -18.925  0.000\nhr[7]                         23.9511    3.969   6.035  0.000\nhr[8]                        127.5199    3.950  32.284  0.000\nhr[9]                         24.4399    3.936   6.209  0.000\nhr[10]                       -12.3407    3.936  -3.135  0.002\nhr[11]                         9.2814    3.945   2.353  0.019\nhr[12]                        41.1417    3.957  10.397  0.000\nhr[13]                        39.8939    3.975  10.036  0.000\nhr[14]                        30.4940    3.991   7.641  0.000\nhr[15]                        35.9445    3.995   8.998  0.000\nhr[16]                        82.3786    3.988  20.655  0.000\nhr[17]                       200.1249    3.964  50.488  0.000\nhr[18]                       173.2989    3.956  43.806  0.000\nhr[19]                        90.1138    3.940  22.872  0.000\nhr[20]                        29.4071    3.936   7.471  0.000\nhr[21]                        -8.5883    3.933  -2.184  0.029\nhr[22]                       -37.0194    3.934  -9.409  0.000\nworkingday                     1.2696    1.784   0.711  0.477\ntemp                         157.2094   10.261  15.321  0.000\nweathersit[cloudy/misty]     -12.8903    1.964  -6.562  0.000\nweathersit[heavy rain/snow] -109.7446   76.667  -1.431  0.152\nweathersit[light rain/snow]  -66.4944    2.965 -22.425  0.000\n\n\n- 두 방법의 모델의 차이\n\n예측값 차이의 제곱합 계산\n거의 0에 근사\n예측력은 동일다하 할 수 있음\n\n\nnp.sum((M_lm.fittedvalues - M2_lm.fittedvalues)**2)\n\n1.6273003592224878e-19\n\n\n- mnth의 [Dec] 계수 계산\n\n[Dec] 의 계수가 없음\n나머지 값들의 음수를 취해 마지막 계수 생성\n합 거의 0에 근사\n\n\ncoef_month = S2[S2.index.str.contains('mnth')]['coef']\nprint(coef_month)\n# 제약조건이 걸린 계수 확인\nmonths = Bike['mnth'].dtype.categories\ncoef_month = pd.concat([coef_month, pd.Series([-coef_month.sum()], index=['mnth[Dec]'])\n])\nprint(coef_month)\nprint(sum(coef_month))\n\nmnth[Jan]     -46.0871\nmnth[Feb]     -39.2419\nmnth[March]   -29.5357\nmnth[April]    -4.6622\nmnth[May]      26.4700\nmnth[June]     21.7317\nmnth[July]     -0.7626\nmnth[Aug]       7.1560\nmnth[Sept]     20.5912\nmnth[Oct]      29.7472\nmnth[Nov]      14.2229\nName: coef, dtype: float64\nmnth[Jan]     -46.0871\nmnth[Feb]     -39.2419\nmnth[March]   -29.5357\nmnth[April]    -4.6622\nmnth[May]      26.4700\nmnth[June]     21.7317\nmnth[July]     -0.7626\nmnth[Aug]       7.1560\nmnth[Sept]     20.5912\nmnth[Oct]      29.7472\nmnth[Nov]      14.2229\nmnth[Dec]       0.3705\ndtype: float64\n1.4210854715202004e-14\n\n\n- mnth(월) 에 대응되는 계수들을 선도표로 연결해서 시각화\n\nfig_month , ax_month = subplots(figsize=(8,8))\nx_month = np.arange(coef_month.shape[0])\nax_month.plot(x_month , coef_month , marker='o', ms=10)\nax_month.set_xticks(x_month)\nax_month.set_xticklabels([l[5] for l in coef_month.index], fontsize\n=20)\nax_month.set_xlabel('Month', fontsize=20)\nax_month.set_ylabel('Coefficient', fontsize=20);\n\n\n\n\n\n\n\n\n- 시간에 대응되는 계수들을 선도표로 연결해서 시각화\n\ncoef_hr = S2[S2.index.str.contains('hr')]['coef']\ncoef_hr = coef_hr.reindex(['hr[{0}]'.format(h) for h in range(23)])\ncoef_hr = pd.concat([coef_hr, pd.Series([-coef_hr.sum()], index=['hr[23]'])\n])\nfig_hr , ax_hr = subplots(figsize=(8,8))\nx_hr = np.arange(coef_hr.shape[0])\nax_hr.plot(x_hr , coef_hr , marker='o', ms=10)\nax_hr.set_xticks(x_hr[::2])\nax_hr.set_xticklabels(range(24)[::2], fontsize =20)\nax_hr.set_xlabel('Hour', fontsize=20)\nax_hr.set_ylabel('Coefficient', fontsize=20);\n\n\n\n\n\n\n\n\n- 포아송 분포를 가정하고 평균을 추정\n\n월별, 시간에 딸ㄴ 계수의 값을 선도표화\n\n\nM_pois = sm.GLM(Y, X2, family=sm.families.Poisson()).fit()\nS_pois = summarize(M_pois)\ncoef_month = S_pois[S_pois.index.str.contains('mnth')]['coef']\ncoef_month = pd.concat([coef_month ,\npd.Series([-coef_month.sum()],\nindex=['mnth[Dec]'])])\ncoef_hr = S_pois[S_pois.index.str.contains('hr')]['coef']\ncoef_hr = pd.concat([coef_hr ,\npd.Series([-coef_hr.sum()],\nindex=['hr[23]'])])\n\nfig_pois , (ax_month , ax_hr) = subplots(1, 2, figsize=(16,8))\nax_month.plot(x_month , coef_month , marker='o', ms=10)\nax_month.set_xticks(x_month)\nax_month.set_xticklabels([l[5] for l in coef_month.index], fontsize\n=20)\nax_month.set_xlabel('Month', fontsize=20)\nax_month.set_ylabel('Coefficient', fontsize=20)\nax_hr.plot(x_hr , coef_hr , marker='o', ms=10)\nax_hr.set_xticklabels(range(24)[::2], fontsize =20)\nax_hr.set_xlabel('Hour', fontsize=20)\nax_hr.set_ylabel('Coefficient', fontsize=20);\n\n/tmp/ipykernel_154232/2887756137.py:20: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax_hr.set_xticklabels(range(24)[::2], fontsize =20)\n\n\n\n\n\n\n\n\n\n- 일반 선형회귀직선과 포아송 분포를 가정한 평균직선을 비교\n\nfig , ax = subplots(figsize=(8, 8))\nax.scatter(M2_lm.fittedvalues ,\nM_pois.fittedvalues ,\ns=20)\nax.set_xlabel('Linear Regression Fit', fontsize=20)\nax.set_ylabel('Poisson Regression Fit', fontsize=20)\nax.axline([0,0], c='black', linewidth=3,\nlinestyle='--', slope=1);"
  },
  {
    "objectID": "posts/1.Basics.html",
    "href": "posts/1.Basics.html",
    "title": "1. Basics",
    "section": "",
    "text": "1. 모형 생성\n- x는 정규분포에서 생성, y의 평균은 x + 50, 정규분포를 따름\n\nx,y의 상관계수\n\n\nimport numpy as np\nx = np.random.normal(size=50)\ny = x + 50 + np.random.normal(loc=0, scale=1, size=50)\nnp.corrcoef(x, y)\n\narray([[1.        , 0.73497261],\n       [0.73497261, 1.        ]])\n\n\n- 정규분포를 따르는 난수 생성 방법\n\nrng는 변수 생성 객체\n시드가 같으면 난수의 값도 같음\nscale : 표준편차, size : 난수 생성 개수\nstandard_normal 표준 정규분포에서 난수 생성\n\n\nrng = np.random.default_rng(1303)\nprint(rng.normal(scale=5, size=10))\n\nrng2 = np.random.default_rng(1303)\nprint(rng2.normal(scale=5, size=10))\n\nrng = np.random.default_rng(3)\ny = rng.standard_normal(10)\nprint(y)\nnp.mean(y), y.mean()\n\n[  4.09482632  -1.07485605 -10.15364596   1.13406146  -4.14030566\n  -4.74859823   0.48740125   5.65355173  -2.51588502  -8.07334198]\n[  4.09482632  -1.07485605 -10.15364596   1.13406146  -4.14030566\n  -4.74859823   0.48740125   5.65355173  -2.51588502  -8.07334198]\n[ 2.04091912 -2.55566503  0.41809885 -0.56776961 -0.45264929 -0.21559716\n -2.01998613 -0.23193238 -0.86521308  3.32299952]\n\n\n(-0.1126795190952861, -0.1126795190952861)\n\n\n- y의 분산을 확인(표본의 크기가 커지면 1에 가까워짐)\n\nprint(np.var(y)), y.var()\nnp.mean((y - y.mean())**2)\n\n2.7243406406465125\n\n\n2.7243406406465125\n\n\n\n\n2. 그래프, 시각화\n- 그래프 구문 : fig, ax\n- 기본 plot : 선으로 연결\n\nfrom matplotlib.pyplot import subplots\n\nfig, ax = subplots(figsize=(8, 8))\nx = rng.standard_normal(100)\ny = rng.standard_normal(100)\nax.plot(x, y);\n\n\n\n\n\n\n\n\n- 그래프 구문 : 산점도 형태로, 'o'\n\nfig, ax = subplots(figsize=(8, 8))\nax.plot(x, y, 'o');\n\n\n\n\n\n\n\n\n- scatter 구문 사용\n\nfig, ax = subplots(figsize=(8, 8))\nax.scatter(x, y, marker='*')\nax.set_xlabel(\"this is the x-axis\")\nax.set_ylabel(\"this is the y-axis\")\nax.set_title(\"Plot of X vs Y\");\n\n\n\n\n\n\n\n\n- 여러개 동시에 시각화\n\n2X3 행렬 plot : axes를 이용해서 각 그래프를 처리\n\n\nfig, axes = subplots(nrows=2, ncols=3, figsize=(15, 5))\naxes[0,1].plot(x, y, 'o')\naxes[1,2].scatter(x, y, marker='+')\nfig\n\n# 바깥으로 그림을 저장함\n# ---------------------\nfig.savefig(\"Figure.png\", dpi=400)\n\n\n\n\n\n\n\n\n- 3차원 등고선 그래프\n\nfig, ax = subplots(figsize=(8, 8))\nx = np.linspace(-np.pi, np.pi, 50)\ny=x\nf = np.multiply.outer(np.cos(y), 1 / (1 + x**2)) # z = f(x,y)\nax.contour(x, y, f);\nax.contour(x, y, f, levels=45);\n\n\n\n\n\n\n\n\n\n\n3. 행렬, 데이터 프레임\n- 행렬 생성\n\nA = np.array(np.arange(16)).reshape((4, 4))\nA\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15]])\n\n\n- 원하는 행,열만 추출하는 방법1\n\nA[[1,3]][:,[0,3]]\n\narray([[ 4,  7],\n       [12, 15]])\n\n\n- 원하는 행,열만 추출하는 방법2\n\nidx = np.ix_([1,3],[0,3])\nprint(idx)\nA[idx]\n\n(array([[1],\n       [3]]), array([[0, 3]]))\n\n\narray([[ 4,  7],\n       [12, 15]])\n\n\n- 데이터 프레임\n\nimport pandas as pd\nAuto = pd.read_csv('Auto.csv')\nAuto\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n\n\n0\n18.0\n8\n307.0\n130\n3504\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n1\n15.0\n8\n350.0\n165\n3693\n11.5\n70\n1\nbuick skylark 320\n\n\n2\n18.0\n8\n318.0\n150\n3436\n11.0\n70\n1\nplymouth satellite\n\n\n3\n16.0\n8\n304.0\n150\n3433\n12.0\n70\n1\namc rebel sst\n\n\n4\n17.0\n8\n302.0\n140\n3449\n10.5\n70\n1\nford torino\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n392\n27.0\n4\n140.0\n86\n2790\n15.6\n82\n1\nford mustang gl\n\n\n393\n44.0\n4\n97.0\n52\n2130\n24.6\n82\n2\nvw pickup\n\n\n394\n32.0\n4\n135.0\n84\n2295\n11.6\n82\n1\ndodge rampage\n\n\n395\n28.0\n4\n120.0\n79\n2625\n18.6\n82\n1\nford ranger\n\n\n396\n31.0\n4\n119.0\n82\n2720\n19.4\n82\n1\nchevy s-10\n\n\n\n\n397 rows × 9 columns\n\n\n\n\nAuto.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 397 entries, 0 to 396\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   mpg           397 non-null    float64\n 1   cylinders     397 non-null    int64  \n 2   displacement  397 non-null    float64\n 3   horsepower    397 non-null    object \n 4   weight        397 non-null    int64  \n 5   acceleration  397 non-null    float64\n 6   year          397 non-null    int64  \n 7   origin        397 non-null    int64  \n 8   name          397 non-null    object \ndtypes: float64(3), int64(4), object(2)\nmemory usage: 28.0+ KB\n\n\n- 구별되는 값 확인 및 결측치 제거\n\n결측치 제거해도 행이 안사라진걸 보니 결측치 없음\n\n\nprint(np.unique(Auto['horsepower']))\n\nAuto_new = Auto.dropna()\nprint(Auto.shape)\nprint(Auto_new.shape)\n\n['100' '102' '103' '105' '107' '108' '110' '112' '113' '115' '116' '120'\n '122' '125' '129' '130' '132' '133' '135' '137' '138' '139' '140' '142'\n '145' '148' '149' '150' '152' '153' '155' '158' '160' '165' '167' '170'\n '175' '180' '190' '193' '198' '200' '208' '210' '215' '220' '225' '230'\n '46' '48' '49' '52' '53' '54' '58' '60' '61' '62' '63' '64' '65' '66'\n '67' '68' '69' '70' '71' '72' '74' '75' '76' '77' '78' '79' '80' '81'\n '82' '83' '84' '85' '86' '87' '88' '89' '90' '91' '92' '93' '94' '95'\n '96' '97' '98' '?']\n(397, 9)\n(397, 9)\n\n\n- 인덱스를 행라벨로 바꾸는 작업\n\n각 행의 이름표를 ’name’변수로 변경\n변수 하나 줄어듦\n\n\nAuto_re = Auto.set_index('name')\nAuto_re\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\n\n\nname\n\n\n\n\n\n\n\n\n\n\n\n\nchevrolet chevelle malibu\n18.0\n8\n307.0\n130\n3504\n12.0\n70\n1\n\n\nbuick skylark 320\n15.0\n8\n350.0\n165\n3693\n11.5\n70\n1\n\n\nplymouth satellite\n18.0\n8\n318.0\n150\n3436\n11.0\n70\n1\n\n\namc rebel sst\n16.0\n8\n304.0\n150\n3433\n12.0\n70\n1\n\n\nford torino\n17.0\n8\n302.0\n140\n3449\n10.5\n70\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nford mustang gl\n27.0\n4\n140.0\n86\n2790\n15.6\n82\n1\n\n\nvw pickup\n44.0\n4\n97.0\n52\n2130\n24.6\n82\n2\n\n\ndodge rampage\n32.0\n4\n135.0\n84\n2295\n11.6\n82\n1\n\n\nford ranger\n28.0\n4\n120.0\n79\n2625\n18.6\n82\n1\n\n\nchevy s-10\n31.0\n4\n119.0\n82\n2720\n19.4\n82\n1\n\n\n\n\n397 rows × 8 columns\n\n\n\n- 데이터 프레임에서 원하는 값 찾는 법\n\nrows = ['amc rebel sst', 'ford torino']\nprint(Auto_re.loc[rows])\nprint(Auto_re.iloc[[1,2,3,4],[0,2,3]])\n\n                mpg  cylinders  displacement horsepower  weight  acceleration  \\\nname                                                                            \namc rebel sst  16.0          8         304.0        150    3433          12.0   \nford torino    17.0          8         302.0        140    3449          10.5   \n\n               year  origin  \nname                         \namc rebel sst    70       1  \nford torino      70       1  \n                     mpg  displacement horsepower\nname                                             \nbuick skylark 320   15.0         350.0        165\nplymouth satellite  18.0         318.0        150\namc rebel sst       16.0         304.0        150\nford torino         17.0         302.0        140\n\n\n- 연도가 80 넘는 것 중 weight 과 origin\n\nidx_80 = Auto_re['year'] &gt; 80\nprint(Auto_re.loc[idx_80, ['weight', 'origin']])\n\n                                   weight  origin\nname                                             \nplymouth reliant                     2490       1\nbuick skylark                        2635       1\ndodge aries wagon (sw)               2620       1\nchevrolet citation                   2725       1\nplymouth reliant                     2385       1\ntoyota starlet                       1755       3\nplymouth champ                       1875       1\nhonda civic 1300                     1760       3\nsubaru                               2065       3\ndatsun 210 mpg                       1975       3\ntoyota tercel                        2050       3\nmazda glc 4                          1985       3\nplymouth horizon 4                   2215       1\nford escort 4w                       2045       1\nford escort 2h                       2380       1\nvolkswagen jetta                     2190       2\nrenault 18i                          2320       2\nhonda prelude                        2210       3\ntoyota corolla                       2350       3\ndatsun 200sx                         2615       3\nmazda 626                            2635       3\npeugeot 505s turbo diesel            3230       2\nvolvo diesel                         3160       2\ntoyota cressida                      2900       3\ndatsun 810 maxima                    2930       3\nbuick century                        3415       1\noldsmobile cutlass ls                3725       1\nford granada gl                      3060       1\nchrysler lebaron salon               3465       1\nchevrolet cavalier                   2605       1\nchevrolet cavalier wagon             2640       1\nchevrolet cavalier 2-door            2395       1\npontiac j2000 se hatchback           2575       1\ndodge aries se                       2525       1\npontiac phoenix                      2735       1\nford fairmont futura                 2865       1\nvolkswagen rabbit l                  1980       2\nmazda glc custom l                   2025       3\nmazda glc custom                     1970       3\nplymouth horizon miser               2125       1\nmercury lynx l                       2125       1\nnissan stanza xe                     2160       3\nhonda accord                         2205       3\ntoyota corolla                       2245       3\nhonda civic                          1965       3\nhonda civic (auto)                   1965       3\ndatsun 310 gx                        1995       3\nbuick century limited                2945       1\noldsmobile cutlass ciera (diesel)    3015       1\nchrysler lebaron medallion           2585       1\nford granada l                       2835       1\ntoyota celica gt                     2665       3\ndodge charger 2.2                    2370       1\nchevrolet camaro                     2950       1\nford mustang gl                      2790       1\nvw pickup                            2130       2\ndodge rampage                        2295       1\nford ranger                          2625       1\nchevy s-10                           2720       1\n\n\n- 연도가 80을 넘고 mpg가 30이 넘는 것 중 weight와 origin\n\nprint(Auto_re.loc[lambda df: (df['year'] &gt; 80) & (df['mpg'] &gt; 30),\n['weight', 'origin'] ])\n\n                                   weight  origin\nname                                             \ntoyota starlet                       1755       3\nplymouth champ                       1875       1\nhonda civic 1300                     1760       3\nsubaru                               2065       3\ndatsun 210 mpg                       1975       3\ntoyota tercel                        2050       3\nmazda glc 4                          1985       3\nplymouth horizon 4                   2215       1\nford escort 4w                       2045       1\nvolkswagen jetta                     2190       2\nrenault 18i                          2320       2\nhonda prelude                        2210       3\ntoyota corolla                       2350       3\ndatsun 200sx                         2615       3\nmazda 626                            2635       3\nvolvo diesel                         3160       2\nchevrolet cavalier 2-door            2395       1\npontiac j2000 se hatchback           2575       1\nvolkswagen rabbit l                  1980       2\nmazda glc custom l                   2025       3\nmazda glc custom                     1970       3\nplymouth horizon miser               2125       1\nmercury lynx l                       2125       1\nnissan stanza xe                     2160       3\nhonda accord                         2205       3\ntoyota corolla                       2245       3\nhonda civic                          1965       3\nhonda civic (auto)                   1965       3\ndatsun 310 gx                        1995       3\noldsmobile cutlass ciera (diesel)    3015       1\ntoyota celica gt                     2665       3\ndodge charger 2.2                    2370       1\nvw pickup                            2130       2\ndodge rampage                        2295       1\nchevy s-10                           2720       1\n\n\n\n\n4. 데이터 처리\n- zip 구문을 이용해서 가중평균 계산\n\ntotal = 0\nfor value, weight in zip([2,3,19],[0.2,0.3,0.5]):\n  total += weight * value\n  print('Weighted average is: {0}'.format(total))\n\nWeighted average is: 0.4\nWeighted average is: 1.2999999999999998\nWeighted average is: 10.8\n\n\n- M에서 np.nan을 이용해 임의로 결측치 생성\n\nrng = np.random.default_rng(1)\nA = rng.standard_normal((127, 5))\nM = rng.choice([0, np.nan], p=[0.8,0.2], size=A.shape)\nA += M\nD = pd.DataFrame(A, columns=['food','bar', 'pickle', 'snack', 'popcorn'])\nprint(D[:3])\n\n       food       bar    pickle     snack   popcorn\n0  0.345584  0.821618  0.330437 -1.303157       NaN\n1       NaN -0.536953  0.581118  0.364572  0.294132\n2       NaN  0.546713       NaN -0.162910 -0.482119\n\n\n- nan값의 비율 확인\n\nfor col in D.columns:\n  template = 'Column \"{0}\" has {1:.2%} missing values'\n  print(template.format(col, np.isnan(D[col]).mean()))\n  #print format 0: col, 1: np.isnan(D[col]).mean()\n  #col-colname\n\nColumn \"food\" has 16.54% missing values\nColumn \"bar\" has 25.98% missing values\nColumn \"pickle\" has 29.13% missing values\nColumn \"snack\" has 21.26% missing values\nColumn \"popcorn\" has 22.83% missing values\n\n\n- 상호 관계를 알기 위한 산점도\n\n마력이 높으면 연비가 좋다\n(차이가 미묘하게 올라가므로 검증 필요)\n\n\nfig, ax = subplots(figsize=(8, 8))\nax.plot(Auto['horsepower'], Auto['mpg'], 'o');"
  },
  {
    "objectID": "posts/2.LinearRegression.html",
    "href": "posts/2.LinearRegression.html",
    "title": "2. Linear Regression",
    "section": "",
    "text": "1. imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import subplots\n\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\nfrom statsmodels.stats.anova import anova_lm\n\n\nfrom ISLP import load_data\n\n\n\n2. 단순 선형 회귀 분석\n- Boston data 불러오기\n\nlstat 변수만 사용\nX = [1, lstat 벡터] 형태\ny = medv 변수\n\n\nfrom ISLP.models import (ModelSpec as MS, summarize , poly)\nfrom pandas.io.formats.format import DataFrameFormatter\n# Inside the pandas/io/formats/html.py file (you'll need to find this file in your pandas installation),\n# locate the _get_columns_formatted_values function and modify it as follows:\ndef _get_columns_formatted_values(self) -&gt; list[str]:\n        # only reached with non-Multi Index\n        # return self.columns._format_flat(include_name=False)  # Replace this line\n  formatter = DataFrameFormatter(self.columns, include_name=False)  # With this line\n  return formatter._format_col_name_split()\n\nBoston = load_data(\"Boston\")\nprint(Boston.columns)\nX = pd.DataFrame({'intercept': np.ones(Boston.shape[0]), 'lstat': Boston['lstat']})\nprint(X[:4])\ny = Boston['medv']\nprint(f'-데이터의 수 : {len(y)}')\n\nIndex(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\n       'ptratio', 'lstat', 'medv'],\n      dtype='object')\n   intercept  lstat\n0        1.0   4.98\n1        1.0   9.14\n2        1.0   4.03\n3        1.0   2.94\n-데이터의 수 : 506\n\n\n- Boston data 단순 선형 회귀 분석\n\nmedv ~ lstat\n\\(R^2\\) 독립변수가 종속변수를 얼마나 잘 설명하는지 0~1사이\n\n독립변수가 늘어날수록 무조건 증가\n\nadj. \\(R^2\\) 변수의 수를 고려해 보정 (단순 선형 회귀에서는 거의 같음)\nDF(Degrees of Freedom, 자유도) 1/504\nDF Residuals : 504 \\(\\to\\) 잔차 자유도(관측값 수 - 추정한 모수 수-보통 n-2)\nDF Model : 1 \\(\\to\\) 회귀 자유도(독립변수 수)\nDurbin-Watson(DW 통계량) : 오차들 사이에 자기상관\n\n2 근처 : 자기상관 없음(좋음)\n2보다 작음 : 양의 자기상관\n2보다 큼 : 음의 자기상관\n\nSkew : 왜도\n\n0 : 완전 대칭\n(&gt; 0) : 오른쪽 꼬리가 길다\n(&lt; 0 ) : 왼쪽 꼬리가 길다\n\nKurtosis : 첨도\n\n3 : 정규분포\n(&gt; 3) : 뾰족하고 꼬리가 두꺼움\n(&lt; 3) : 평평하고 꼬리가 얇음\n\n\n\n- 덜 중요?\n\nF-statistic : 전체 모델이 통계적으로 유의미한지\nProb : F값에 따른 p-value\nintercept : 절편\n\ncoef : lstat = 0 일 때 medv가 34.55로 예측된다는 뜻\nstd err : 표준오차\nt : t-통계량 \\(\\to\\) 계수가 0인지 아닌지 검정\nP&gt;|t| : p-value \\(\\to\\) 작을수록 유의미한 변수\n[0.025, 0.975] : 95% 신뢰구간\n\nlstat\n\ncoef : lstat이 1% 증가할 때 medv는 약 0.95감소\n\nOmnibus : 정규성 검정 \\(\\to\\) 클수록 정규성 위배 가능성 커짐\nProb(Omnibus) : p-value 0.000 이므로 정규분포 아닐 가능성 큼\nJarque-Bera : 또다른 정규성 검정 지표\nLog-Likelihood : 모델의 로그 가능도\nAIC / BIC : 모형 간 비교 지표(낮을 수록 좋음)\nCond. No : 다중공선성 지표(높으면 위험)\n\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   medv   R-squared:                       0.544\nModel:                            OLS   Adj. R-squared:                  0.543\nMethod:                 Least Squares   F-statistic:                     601.6\nDate:                Sun, 06 Apr 2025   Prob (F-statistic):           5.08e-88\nTime:                        09:10:46   Log-Likelihood:                -1641.5\nNo. Observations:                 506   AIC:                             3287.\nDf Residuals:                     504   BIC:                             3295.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     34.5538      0.563     61.415      0.000      33.448      35.659\nlstat         -0.9500      0.039    -24.528      0.000      -1.026      -0.874\n==============================================================================\nOmnibus:                      137.043   Durbin-Watson:                   0.892\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              291.373\nSkew:                           1.453   Prob(JB):                     5.36e-64\nKurtosis:                       5.319   Cond. No.                         29.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n- _X : 아무런 변수 생성 : p-value 0.05보다 작게 나올수도?\n\nT-test로 변수 선택하는데에는 한계 존재, 변수가 많을 때는 다른 방법\n\n\n_X =np.random.normal(0,1, size=(len(y), len(y-1))) \n_X\n\narray([[-1.2014418 , -0.36229144, -0.20156353, ...,  0.5128146 ,\n        -1.69055217,  1.36297382],\n       [ 1.97238504, -0.22323065,  0.51710176, ...,  0.81227357,\n        -0.85792917,  0.27368867],\n       [-0.74635151, -0.2920941 , -0.8552335 , ...,  1.22772083,\n        -0.07547889, -0.59557387],\n       ...,\n       [ 0.20602294,  0.13454536,  0.72263167, ...,  0.03738331,\n         0.49048762, -0.44020111],\n       [-0.07030914, -0.04136855,  0.16204887, ...,  0.23999928,\n         1.4415386 ,  0.69481763],\n       [ 0.30221035, -0.50379848, -0.81854603, ...,  0.47187981,\n        -0.95241703,  0.0196086 ]])\n\n\n- MS 구문 이용해 입력 행렬 처리\n\ndesign = MS(['lstat'])\ndesign = design.fit(Boston)\nX = design.transform(Boston)\nprint(X[:4])\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.params)\n\n   intercept  lstat\n0        1.0   4.98\n1        1.0   9.14\n2        1.0   4.03\n3        1.0   2.94\nintercept    34.553841\nlstat        -0.950049\ndtype: float64\n\n\n- 새로운 입력변수 [1 , lstat벡터] 생성, 예측\n\nnew_df = pd.DataFrame({'lstat':[5, 10, 15]})\nnewX = design.transform(new_df)\nprint(newX)\nnew_predictions = results.get_prediction(newX);\nprint(f'평균 : \\n{new_predictions.predicted_mean}')\nprint(f'신뢰구간 : \\n{new_predictions.conf_int(alpha=0.05)}')\nprint(f'예측구간 : \\n{new_predictions.conf_int(obs=True, alpha=0.05)}')\n\n   intercept  lstat\n0        1.0      5\n1        1.0     10\n2        1.0     15\n평균 : \n[29.80359411 25.05334734 20.30310057]\n신뢰구간 : \n[[29.00741194 30.59977628]\n [24.47413202 25.63256267]\n [19.73158815 20.87461299]]\n예측구간 : \n[[17.56567478 42.04151344]\n [12.82762635 37.27906833]\n [ 8.0777421  32.52845905]]\n\n\n- 평균직선에 대한 그래프 / 잔차에 대한 그래프\n\nimport matplotlib.pyplot as plt\n\ndef abline(ax, b, m):\n  xlim = ax.get_xlim()\n  ylim = [m * xlim[0] + b, m * xlim[1] + b]\n  ax.plot(xlim, ylim)\n\ndef abline(ax, b, m, *args, **kwargs):\n  xlim = ax.get_xlim()\n  ylim = [m * xlim[0] + b, m * xlim[1] + b]\n  ax.plot(xlim, ylim, *args, **kwargs)\n\nax = subplots(figsize=(5,5))[1]\nabline(ax,\nresults.params[0],\nresults.params[1], 'r--', linewidth=3)\n\nax = subplots(figsize=(8,8))[1]\nax.scatter(results.fittedvalues, results.resid)\nax.set_xlabel('Fitted value')\nax.set_ylabel('Residual')\nax.axhline(0, c='k', ls='--');\n\n/tmp/ipykernel_147714/1347888005.py:15: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  results.params[0],\n/tmp/ipykernel_147714/1347888005.py:16: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  results.params[1], 'r--', linewidth=3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n- 잔차 표준화 필요 : 0~1사이 왔다갔다..\n- 현재 문제점 : 잔차들이 곡선의 형태를 띄고 있음 \\(\\to\\) 곡선을 표현할 수 있는 변수 사용\n- 관측치에 대한 leverage(각 관측치의 영향력) 계산 및 그래프화\n\n각 데이터마다 영향의 크기가 다름\n\n\ninfl = results.get_influence()\nax = subplots(figsize=(8,8))[1]\nax.scatter(np.arange(X.shape[0]), infl.hat_matrix_diag)\nax.set_xlabel('Index')\nax.set_ylabel('Leverage')\nnp.argmax(infl.hat_matrix_diag)\n\n374\n\n\n\n\n\n\n\n\n\n\n\n3. 다중 선형 회귀\n- 입력 변수를 lstat + age로 변경\n\nX = MS(['lstat', 'age']).fit_transform(Boston)\nmodel1 = sm.OLS(y, X)\nresults1 = model1.fit()\nprint(summarize(results1))\n\n              coef  std err       t  P&gt;|t|\nintercept  33.2228    0.731  45.458  0.000\nlstat      -1.0321    0.048 -21.416  0.000\nage         0.0345    0.012   2.826  0.005\n\n\n- 입력변수를 mdev를 제외한 모든 변수로 확장\n\nterms = Boston.columns.drop('medv')\nterms\nX = MS(terms).fit_transform(Boston)\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(summarize(results))\n\n              coef  std err       t  P&gt;|t|\nintercept  41.6173    4.936   8.431  0.000\ncrim       -0.1214    0.033  -3.678  0.000\nzn          0.0470    0.014   3.384  0.001\nindus       0.0135    0.062   0.217  0.829\nchas        2.8400    0.870   3.264  0.001\nnox       -18.7580    3.851  -4.870  0.000\nrm          3.6581    0.420   8.705  0.000\nage         0.0036    0.013   0.271  0.787\ndis        -1.4908    0.202  -7.394  0.000\nrad         0.2894    0.067   4.325  0.000\ntax        -0.0127    0.004  -3.337  0.001\nptratio    -0.9375    0.132  -7.091  0.000\nlstat      -0.5520    0.051 -10.897  0.000\n\n\n- p-value가 가장 높은 indus, age 변수 제외\n\nminus_age = Boston.columns.drop(['medv','indus', 'age'])\nXma = MS(minus_age).fit_transform(Boston)\nmodel1 = sm.OLS(y, Xma)\nprint(summarize(model1.fit()))\n\n              coef  std err       t  P&gt;|t|\nintercept  41.4517    4.903   8.454  0.000\ncrim       -0.1217    0.033  -3.696  0.000\nzn          0.0462    0.014   3.378  0.001\nchas        2.8719    0.863   3.329  0.001\nnox       -18.2624    3.565  -5.122  0.000\nrm          3.6730    0.409   8.978  0.000\ndis        -1.5160    0.188  -8.078  0.000\nrad         0.2839    0.064   4.440  0.000\ntax        -0.0123    0.003  -3.608  0.000\nptratio    -0.9310    0.130  -7.138  0.000\nlstat      -0.5465    0.047 -11.519  0.000\n\n\n- 변수 제외하고 중간에서도 전체 요약을 출력해 전체적인 변화를 봐야함\n\nresult1 = model1.fit()\n#print(result1.summary())\n\n- 다중공선성을 확인하기 위한 VIF값\n\n값이 크다는 것은 \\(R^2\\)가 크다는 것\n\\(\\to\\) 자신이 없어도 나머지 변수들의 영향을 많이 받아 설명을 잘한다는 의미\n\n\nvals = [VIF(X, i) for i in range(1, X.shape[1])]\nvif = pd.DataFrame({'vif':vals},index=X.columns[1:])\nprint(vif)\n\n              vif\ncrim     1.767486\nzn       2.298459\nindus    3.987181\nchas     1.071168\nnox      4.369093\nrm       1.912532\nage      3.088232\ndis      3.954037\nrad      7.445301\ntax      9.002158\nptratio  1.797060\nlstat    2.870777\n\n\n- 교호작용을 포함한 선형회귀분석\n\n교호작용이라는 두 변수 간의 시너지 존재하는 경우!\n특정 변수가 다른 변수의 영향력에 영향을 준다는 의미\n변수가 많으면 모든 교호작용 고려 힘듦\nEDA먼저 필요\n\n- lstat과 age간의교호작용\n\n결과 : lstat이 평균을 높여주는 효과가 age가 클수록 커짐\n\n\nX = MS(['lstat', 'age', ('lstat', 'age')]).fit_transform(Boston)\nmodel2 = sm.OLS(y, X)\nprint(summarize(model2.fit()))\n\n              coef  std err       t  P&gt;|t|\nintercept  36.0885    1.470  24.553  0.000\nlstat      -1.3921    0.167  -8.313  0.000\nage        -0.0007    0.020  -0.036  0.971\nlstat:age   0.0042    0.002   2.244  0.025\n\n\n- lstat에 대해 다항식 차원을 2까지 늘림\n\nlstat + lstat\\(^2\\)\n만약 5차원이면 lstat + lstat\\(^2\\) + … + lstat\\(^5\\)\n아래에서 2차식까지 통계적 유의성 확인됨\n\n\nX = MS([poly('lstat', degree=2), 'age']).fit_transform(Boston)\nmodel3 = sm.OLS(y, X)\nresults3 = model3.fit()\nprint(summarize(results3))\n\n                              coef  std err       t  P&gt;|t|\nintercept                  17.7151    0.781  22.681    0.0\npoly(lstat, degree=2)[0] -179.2279    6.733 -26.620    0.0\npoly(lstat, degree=2)[1]   72.9908    5.482  13.315    0.0\nage                         0.0703    0.011   6.471    0.0\n\n\n- result1은 lstat에 대한 선형, result3은 lstat에 대한 2차 다항식\n\n아래 검정은 다항식을 2차로 쓰는 것이 유의한지를 검정\n0,1 둘 다 유의하다는 결과 \\(\\to\\) 그러면 2차식까지 쓰는 것이 맞음\n큰 모형에서 확실한 효과가 있다는 것이 보여짐\n\n\nprint(anova_lm(results1, results3))\n\n   df_resid           ssr  df_diff      ss_diff           F        Pr(&gt;F)\n0     503.0  19168.128609      0.0          NaN         NaN           NaN\n1     502.0  14165.613251      1.0  5002.515357  177.278785  7.468491e-35\n\n\n- results3의 결과에 대한 잔차의 산점도\n\nax = subplots(figsize=(8,8))[1]\nax.scatter(results3.fittedvalues, results3.resid)\nax.set_xlabel('Fitted value')\nax.set_ylabel('Residual')\nax.axhline(0, c='k', ls='--')\n\n\n\n\n\n\n\n\n\n- Carseats data\n\nSales 반응변수\n나머지 전체 + IncomeAdvertising과 PriceAge의 교호작용 추가\n둘 중에서 Income*advertising만 통계적으로 유의\n\n\nCarseats = load_data('Carseats')\nprint(Carseats.columns)\nprint(Carseats.head())\nallvars = list(Carseats.columns.drop('Sales'))\ny = Carseats['Sales']\n\nfinal = allvars + [('Income', 'Advertising'),('Price', 'Age')]\nX = MS(final).fit_transform(Carseats)\nmodel = sm.OLS(y, X)\nprint(model.fit().aic)\nprint(summarize(model.fit()))\n\nIndex(['Sales', 'CompPrice', 'Income', 'Advertising', 'Population', 'Price',\n       'ShelveLoc', 'Age', 'Education', 'Urban', 'US'],\n      dtype='object')\n   Sales  CompPrice  Income  Advertising  Population  Price ShelveLoc  Age  \\\n0   9.50        138      73           11         276    120       Bad   42   \n1  11.22        111      48           16         260     83      Good   65   \n2  10.06        113      35           10         269     80    Medium   59   \n3   7.40        117     100            4         466     97    Medium   55   \n4   4.15        141      64            3         340    128       Bad   38   \n\n   Education Urban   US  \n0         17   Yes  Yes  \n1         10   Yes  Yes  \n2         12   Yes  Yes  \n3         14   Yes  Yes  \n4         13   Yes   No  \n1157.337779308029\n                      coef  std err       t  P&gt;|t|\nintercept           6.5756    1.009   6.519  0.000\nCompPrice           0.0929    0.004  22.567  0.000\nIncome              0.0109    0.003   4.183  0.000\nAdvertising         0.0702    0.023   3.107  0.002\nPopulation          0.0002    0.000   0.433  0.665\nPrice              -0.1008    0.007 -13.549  0.000\nShelveLoc[Good]     4.8487    0.153  31.724  0.000\nShelveLoc[Medium]   1.9533    0.126  15.531  0.000\nAge                -0.0579    0.016  -3.633  0.000\nEducation          -0.0209    0.020  -1.063  0.288\nUrban[Yes]          0.1402    0.112   1.247  0.213\nUS[Yes]            -0.1576    0.149  -1.058  0.291\nIncome:Advertising  0.0008    0.000   2.698  0.007\nPrice:Age           0.0001    0.000   0.801  0.424\n\n\n\n\n4. 가변수 사용법\n\nfrom ISLP.models import contrast\n\nBike = load_data('Bikeshare')\nBike.shape, Bike.columns\n\n((8645, 15),\n Index(['season', 'mnth', 'day', 'hr', 'holiday', 'weekday', 'workingday',\n        'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'casual',\n        'registered', 'bikers'],\n       dtype='object'))\n\n\n\nX2 = MS(['mnth', 'hr', 'workingday','temp', 'weathersit']).fit_transform(Bike)\nY = Bike['bikers']\nM1_lm = sm.OLS(Y, X2).fit()\nS2 = summarize(M1_lm)\nprint(S2)\n\n                                 coef  std err       t  P&gt;|t|\nintercept                    -68.6317    5.307 -12.932  0.000\nmnth[Feb]                      6.8452    4.287   1.597  0.110\nmnth[March]                   16.5514    4.301   3.848  0.000\nmnth[April]                   41.4249    4.972   8.331  0.000\nmnth[May]                     72.5571    5.641  12.862  0.000\nmnth[June]                    67.8187    6.544  10.364  0.000\nmnth[July]                    45.3245    7.081   6.401  0.000\nmnth[Aug]                     53.2430    6.640   8.019  0.000\nmnth[Sept]                    66.6783    5.925  11.254  0.000\nmnth[Oct]                     75.8343    4.950  15.319  0.000\nmnth[Nov]                     60.3100    4.610  13.083  0.000\nmnth[Dec]                     46.4577    4.271  10.878  0.000\nhr[1]                        -14.5793    5.699  -2.558  0.011\nhr[2]                        -21.5791    5.733  -3.764  0.000\nhr[3]                        -31.1408    5.778  -5.389  0.000\nhr[4]                        -36.9075    5.802  -6.361  0.000\nhr[5]                        -24.1355    5.737  -4.207  0.000\nhr[6]                         20.5997    5.704   3.612  0.000\nhr[7]                        120.0931    5.693  21.095  0.000\nhr[8]                        223.6619    5.690  39.310  0.000\nhr[9]                        120.5819    5.693  21.182  0.000\nhr[10]                        83.8013    5.705  14.689  0.000\nhr[11]                       105.4234    5.722  18.424  0.000\nhr[12]                       137.2837    5.740  23.916  0.000\nhr[13]                       136.0359    5.760  23.617  0.000\nhr[14]                       126.6361    5.776  21.923  0.000\nhr[15]                       132.0865    5.780  22.852  0.000\nhr[16]                       178.5206    5.772  30.927  0.000\nhr[17]                       296.2670    5.749  51.537  0.000\nhr[18]                       269.4409    5.736  46.976  0.000\nhr[19]                       186.2558    5.714  32.596  0.000\nhr[20]                       125.5492    5.704  22.012  0.000\nhr[21]                        87.5537    5.693  15.378  0.000\nhr[22]                        59.1226    5.689  10.392  0.000\nhr[23]                        26.8376    5.688   4.719  0.000\nworkingday                     1.2696    1.784   0.711  0.477\ntemp                         157.2094   10.261  15.321  0.000\nweathersit[cloudy/misty]     -12.8903    1.964  -6.562  0.000\nweathersit[heavy rain/snow] -109.7446   76.667  -1.431  0.152\nweathersit[light rain/snow]  -66.4944    2.965 -22.425  0.000\n\n\n- contrast : 범주형 변수에 대한 코딩방식 지정\n\nsum은 범주혀 변수의 모든 계수의 합이 0이 되도록 제약\n그러면 마지막 범주는 계수 추정되지 않고 나머지 계수들의 합의 음수가 됨\n결과가 0에 가까움 \\(\\to\\) 거의 차이가 없음, 예측력 자체는 인코딩 방식에 거의 영향을 받지 않음\n\n\nhr_encode = contrast('hr', 'sum')     # 합에 대한 제약을 검\nmnth_encode = contrast('mnth', 'sum') # 합에 대한 제약을 검\nX2 = MS([mnth_encode, hr_encode, 'workingday','temp', 'weathersit']).fit_transform(Bike)\nY = Bike['bikers']\nM2_lm = sm.OLS(Y, X2).fit()\nS2 = summarize(M2_lm)\nprint(S2)\nnp.sum((M1_lm.fittedvalues - M2_lm.fittedvalues)**2)\n\n                                 coef  std err       t  P&gt;|t|\nintercept                     73.5974    5.132  14.340  0.000\nmnth[Jan]                    -46.0871    4.085 -11.281  0.000\nmnth[Feb]                    -39.2419    3.539 -11.088  0.000\nmnth[March]                  -29.5357    3.155  -9.361  0.000\nmnth[April]                   -4.6622    2.741  -1.701  0.089\nmnth[May]                     26.4700    2.851   9.285  0.000\nmnth[June]                    21.7317    3.465   6.272  0.000\nmnth[July]                    -0.7626    3.908  -0.195  0.845\nmnth[Aug]                      7.1560    3.535   2.024  0.043\nmnth[Sept]                    20.5912    3.046   6.761  0.000\nmnth[Oct]                     29.7472    2.700  11.019  0.000\nmnth[Nov]                     14.2229    2.860   4.972  0.000\nhr[0]                        -96.1420    3.955 -24.307  0.000\nhr[1]                       -110.7213    3.966 -27.916  0.000\nhr[2]                       -117.7212    4.016 -29.310  0.000\nhr[3]                       -127.2828    4.081 -31.191  0.000\nhr[4]                       -133.0495    4.117 -32.319  0.000\nhr[5]                       -120.2775    4.037 -29.794  0.000\nhr[6]                        -75.5424    3.992 -18.925  0.000\nhr[7]                         23.9511    3.969   6.035  0.000\nhr[8]                        127.5199    3.950  32.284  0.000\nhr[9]                         24.4399    3.936   6.209  0.000\nhr[10]                       -12.3407    3.936  -3.135  0.002\nhr[11]                         9.2814    3.945   2.353  0.019\nhr[12]                        41.1417    3.957  10.397  0.000\nhr[13]                        39.8939    3.975  10.036  0.000\nhr[14]                        30.4940    3.991   7.641  0.000\nhr[15]                        35.9445    3.995   8.998  0.000\nhr[16]                        82.3786    3.988  20.655  0.000\nhr[17]                       200.1249    3.964  50.488  0.000\nhr[18]                       173.2989    3.956  43.806  0.000\nhr[19]                        90.1138    3.940  22.872  0.000\nhr[20]                        29.4071    3.936   7.471  0.000\nhr[21]                        -8.5883    3.933  -2.184  0.029\nhr[22]                       -37.0194    3.934  -9.409  0.000\nworkingday                     1.2696    1.784   0.711  0.477\ntemp                         157.2094   10.261  15.321  0.000\nweathersit[cloudy/misty]     -12.8903    1.964  -6.562  0.000\nweathersit[heavy rain/snow] -109.7446   76.667  -1.431  0.152\nweathersit[light rain/snow]  -66.4944    2.965 -22.425  0.000\n\n\n1.6273003592224878e-19\n\n\n- 제약조건이 걸린 계수 확인\n\n위의 계수들의 합에 음수를 취해 마지막 계수 생성\n\n\ncoef_month = S2[S2.index.str.contains('mnth')]['coef']\nprint(coef_month)\nmonths = Bike['mnth'].dtype.categories\ncoef_month = pd.concat([coef_month, pd.Series([-coef_month.sum()], index=['mnth[Dec]'])\n])\nprint(coef_month)\n\nmnth[Jan]     -46.0871\nmnth[Feb]     -39.2419\nmnth[March]   -29.5357\nmnth[April]    -4.6622\nmnth[May]      26.4700\nmnth[June]     21.7317\nmnth[July]     -0.7626\nmnth[Aug]       7.1560\nmnth[Sept]     20.5912\nmnth[Oct]      29.7472\nmnth[Nov]      14.2229\nName: coef, dtype: float64\nmnth[Jan]     -46.0871\nmnth[Feb]     -39.2419\nmnth[March]   -29.5357\nmnth[April]    -4.6622\nmnth[May]      26.4700\nmnth[June]     21.7317\nmnth[July]     -0.7626\nmnth[Aug]       7.1560\nmnth[Sept]     20.5912\nmnth[Oct]      29.7472\nmnth[Nov]      14.2229\nmnth[Dec]       0.3705\ndtype: float64\n\n\n- 월에 대응되는 계수들을 선도표로 연결해서 시각화\n\n겨울에는 잘 안타는 듯\n여름에도 조금 떨어지는 경향을 보임\n\n\nfig_month , ax_month = subplots(figsize=(8,8))\nx_month = np.arange(coef_month.shape[0])\nax_month.plot(x_month , coef_month , marker='o', ms=10)\nax_month.set_xticks(x_month)\nax_month.set_xticklabels([l[5] for l in coef_month.index], fontsize\n=20)\nax_month.set_xlabel('Month', fontsize=20)\nax_month.set_ylabel('Coefficient', fontsize=20);\n\n\n\n\n\n\n\n\n- 시간에 대응되는 계수들을 선도표로 연겨해서 시각화\n\ncoef_hr = S2[S2.index.str.contains('hr')]['coef']\ncoef_hr = coef_hr.reindex(['hr[{0}]'.format(h) for h in range(23)])\ncoef_hr = pd.concat([coef_hr, pd.Series([-coef_hr.sum()], index=['hr[23]'])\n])\nfig_hr , ax_hr = subplots(figsize=(8,8))\nx_hr = np.arange(coef_hr.shape[0])\nax_hr.plot(x_hr , coef_hr , marker='o', ms=9)\nax_hr.set_xticks(x_hr[::2])\nax_hr.set_xticklabels(range(24)[::2], fontsize =20)\nax_hr.set_xlabel('Hour', fontsize=20)\nax_hr.set_ylabel('Coefficient', fontsize=20);"
  }
]